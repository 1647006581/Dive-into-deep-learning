# <u>Dive-Into-Deeplearningå­¦ä¹ ç¬”è®° </u>

# çº¿æ€§å›å½’

Â Â Â Â Â Â Â Â çº¿æ€§å›å½’æ˜¯ä¸€ç§ç®€å•çš„å›å½’æ¨¡å‹ï¼Œå…¶ä½œç”¨æ˜¯é¢„æµ‹ä¸€ä¸ªçº¿æ€§çš„è¿ç»­

<img title="" src="./images/2024-01-26-15-40-08-image.png" alt="" width="590">

## çº¿æ€§æ¨¡å‹

Â Â Â Â Â Â Â Â å‡è®¾nç»´è¾“å…¥å‘é‡
        $$ 
        \mathbf{x} = [x_1, x_2, \ldots, x_n]^T 
        $$

Â Â Â Â Â Â Â Â è¾“å‡ºç»“æœä¸º $\mathbf{y}=w_1 * x_1 + w_2 *x_2 + ...+w_n*x_n + b$    

Â Â Â Â Â Â Â Â $w$è¡¨ç¤º weight æƒé‡   $b$è¡¨ç¤ºbias åç§»

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â å…¶ä¸­Â $\mathbf{w} = [w_1, w_2, \ldots, w_n]^T$

Â Â Â Â Â Â Â Â Â è¿™é‡Œ çº¿æ€§å›å½’è¦åšçš„ä»»åŠ¡å°±æ˜¯é€šè¿‡å·²æœ‰çš„ä¸€äº›ç¦»æ•£çš„ç‚¹å»æ‹Ÿåˆå‡ºä¸€ç»„$\mathbf{w},b$ å¾—åˆ°ä¸€æ¡æ›²çº¿ç”¨ä»¥é¢„æµ‹å…³äºå¯¹åº”ç‰¹å¾çš„å…³ç³»

Â Â Â Â Â Â Â Â $\hat{y}=<\mathbf{w},\mathbf{x}>+  b$

## è®­ç»ƒæ•°æ®

Â Â Â Â Â Â Â Â å‡è®¾æœ‰nä¸ªæ ·æœ¬è®°ä¸º:(<mark>æ³¨æ„è¿™é‡Œ$X$ å’Œå‰é¢æ‰€è¿°çš„å‘é‡ $x$ä¸ä¸€æ · </mark>ï¼Œ<mark>æ­¤å¤„ä¸ºæ ·æœ¬ å¯ä»¥ç†è§£ä¸ºä¸‹æ–¹çš„$x_i$ä¸ºä¸€ä¸ªå‰é¢è¯´çš„å‘é‡$x$)</mark>
Â Â Â Â Â Â Â Â $X = [x_1, x_2, â€¦, x_n]^T  ,y = [y_1, y_2, â€¦, y_n]^T$

## æŸå¤±å‡½æ•°

Â Â Â Â Â Â Â è¯„ä¼°æ¨¡å‹è®­ç»ƒå¥½åæœ€ç›´è§‚çš„æ–¹å¼å°±æ˜¯æŸ¥çœ‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®å€¼

Â Â Â Â Â Â Â Â Â Â Â Â å¹³æ–¹æŸå¤±å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š~~(äºŒåˆ†ä¹‹ä¸€æ˜¯å› ä¸ºå¯ä»¥æ±‚å¯¼æ¶ˆå»ï¼Œå…¶å®æ— æ‰€è°“æ˜¯å¤šå°‘)~~

Â Â Â Â Â Â Â Â Â Â Â Â $â„“(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2$

## ä¼˜åŒ–ç®—æ³•

Â Â Â Â Â Â Â Â æœ‰äº†æ¨¡å‹å’ŒæŸå¤±å‡½æ•°ï¼Œç›¸å½“äºæœ‰äº†ææ–™å’Œå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸå¤±å‡½æ•°è¯„ä¼°ç›®å‰æ¨¡å‹è®­ç»ƒå¥½åï¼Œä½†æ˜¯è¿˜éœ€è¦ä¸€ä¸ªè¾¾åˆ°æˆ‘ä»¬å¯æ¥å—çš„æŸå¤±çš„æ–¹æ³•  ~~ï¼ˆä¸»è§‚èƒ½åŠ¨æ€§ï¼‰~~ï¼Œ

Â Â Â Â Â Â Â Â å¯¹äºçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬å›åˆ°æŸå¤±å‡½æ•°Â Â Â Â $â„“(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2$

Â Â Â Â Â Â Â Â ä¹Ÿå°±æ˜¯$\frac{1}{2} (\hat{y} - y)^2 = \frac{1}{2} (w_1 * x_1 + w_2 *x_2 + ...+w_n*x_n + b - y)^2$

Â Â Â Â Â Â Â Â ä¸ºäº†æ–¹ä¾¿æ¨å¯¼,ç®€å†™ä¸ºå†…ç§¯æ ¼å¼$\frac{1}{2} (\hat{y} - y)^2 = \frac{1}{2} (<\mathit{\mathbf{x}},\mathbf{w}> + b - y)^2$

Â Â Â Â Â Â Â Â Â 

Â Â Â Â Â Â Â Â æœ‰äº†è®­ç»ƒæ•°æ® ä»¥åŠç­‰å¾…é¢„æµ‹å‚æ•°$\mathbf{w}$å’Œ$b$ï¼ŒæŸå¤±å‡½æ•°å¯ä»¥å…·ä½“å†™ä¸ºï¼š

Â Â Â Â Â Â Â Â $â„“(\hat{y}, y) = â„“(\mathbf{X},\mathbf{w},b,\mathbf{y} )$

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â $=\frac{1}{2n}\sum_{i=1}^{n}{(<\mathit{\mathbf{x_i}},\mathbf{w}> + b - y_i)^2}$ 

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â $=\frac{1}{2n}\|\mathbf{X}\mathbf{w}+b-\mathbf{y}  \|^2$

>  Â Â Â Â Â Â Â Â ä¸ºä»€ä¹ˆè¦â—n ï¼Ÿ ä¸ºäº†è·å¾—å¹³å‡æŸå¤±ï¼Œåœ¨è¯„ä¼°ç²¾åº¦çš„æ—¶å€™å¥½ä¸€äº›ã€‚

Â Â Â Â Â Â Â Â æ­¤æ—¶æˆ‘ä»¬å¾—åˆ°æœ€ç»ˆçš„æŸå¤±å‡½æ•°äº†ï¼Œå› ä¸ºæˆ‘ä»¬è¦è®­ç»ƒå‡º$\mathbf{w}$å’Œ$b$çš„å€¼ï¼Œä¼˜åŒ–æ˜¯é’ˆå¯¹è¿™ä¸¤ä¸ªå‚æ•°çš„ï¼Œä¸è¦é™·å…¥æ€ç»´å®šåŠ¿ç›¯ç€*X* å’Œ *y*ã€‚

> Â Â Â Â Â Â Â Â è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼Œè®­ç»ƒå¹¶æ‰¾å‡ºå‡ºä¸€å¯¹ç²¾åº¦å¯ä»¥æ¥å—çš„$\mathbf{w}$å’Œ$b$ï¼Œç”¨äºæ‹Ÿåˆ**y**ä¸**X**çš„çº¿æ€§å…³ç³»Â 

### GDæ¢¯åº¦ä¸‹é™ç®—æ³•

Â Â Â Â Â Â Â Â è§‚å¯ŸæŸå¤±å‡½æ•°ï¼Œ$\ell$  å…³äº$w$çš„åå¯¼ï¼Œ$\ell$ å…³äº$b$çš„åå¯¼  ~~(å¤ªéº»çƒ¦ä¸åŠ ç²—äº†ï¼Œè€Œä¸”ä¸åº”è¯¥å«åå¯¼åº”è¯¥å«æ¢¯åº¦)~~

Â Â Â Â Â Â Â Â æŠŠè¿™ä¸¤ä¸ªå‘é‡çœ‹æˆä¸¤ä¸ªå˜é‡è¯ï¼Œå¯ä»¥å¾—åˆ°$\ell$æœ‰åªä¸€ä¸ªæå€¼ç‚¹ï¼Œä¸ç”¨åˆ¤æ–­äº†åªèƒ½æ˜¯è¿™ä¸ªç‚¹äº†ï¼Œæ‰€ä»¥ç°åœ¨çš„ç›®æ ‡å°±æ˜¯ä¸æ–­é€¼è¿‘è¿™ä¸ªâ€œç‚¹â€

> Â Â Â Â Â Â Â Â è¿™é‡Œæ˜¯æ¨å¯¼è¿‡ç¨‹ï¼š
> 
> ![ ](images\2024-01-26-18-00-42-image.png)

Â Â Â Â Â Â Â Â æ¥ä¸‹æ¥å°±å¼•å‡ºäº†æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨$w$ä¸¾ä¾‹ï¼Œé€‰å®šä¸€ä¸ªåˆå§‹å€¼$w_0$ æ­¤æ—¶ä¸€å®šæ˜¯å’Œè¿™ä¸ªæœ€ä¼˜æˆ–è€…è¯´è¾ƒä¼˜è§£æ˜¯æ¯”è¾ƒè¿œçš„ï¼Œè€Œå¿«é€Ÿå‘è§£é€¼è¿‘çš„æ–¹å‘å°±æ˜¯æ¢¯åº¦æ–¹å‘ï¼ˆæ¢¯åº¦æ€§è´¨ï¼‰ï¼Œæ­¤æ—¶æˆ‘ä»¬åªéœ€è¦ä¸æ–­ç”¨ä¸Šä¸€ä¸ªå‘é‡$w$å‡å»å½“å‰å‘é‡$w$

> æ¢¯åº¦çš„æ–¹å‘æŒ‡å‘å‡½æ•°åœ¨ç»™å®šç‚¹ä¸Šå¢åŠ æœ€å¿«çš„æ–¹å‘ã€‚æ¢¯åº¦çš„åæ–¹å‘æŒ‡å‘å‡½æ•°å‡å°æœ€å¿«çš„æ–¹å‘ã€‚

> <img title="" src="images/2024-01-26-18-02-27-image.png" alt="" width="491">
> 
> å°†è¿‡ç¨‹ç”»å‡ºæ¥ï¼š
> 
> <img title="" src="images/2024-01-26-18-24-51-image.png" alt="" width="378">

Â Â Â Â Â Â Â Â å½“ç„¶ï¼Œä¸ºäº†è¿­ä»£å¯æ§ï¼Œå¯ä»¥å¼•å…¥ä¸€ä¸ªå‚æ•°ï¼Œå­¦ä¹ ç‡æ¥æ§åˆ¶æ­¥é•¿ã€‚å­¦ä¹ ç‡è¿‡å¤§è¿‡å°éƒ½å¯èƒ½å¼•å‘é—®é¢˜ï¼Œå­¦ä¹ ç‡è¿‡å°å¯èƒ½ä¼šå¯¼è‡´è®¡ç®—é‡è¿‡å¤§ä¼šå ç”¨å¤§é‡çš„èµ„æºï¼Œå­¦ä¹ ç‡è¿‡å¤§å¯èƒ½ä¼šå› ä¸ºæ­¥é•¿è¿‡å¤§æ— æ³•è·å¾—è¾ƒç²¾ç¡®è§£ï¼Œæˆ–è€…æ˜¯å¯¼è‡´æ±‚å¯¼è¿‡ç¨‹ä¸­å‡ºç°â—0ã€‚

## å…³é”®ä»£ç å®ç°

pytorchå®ç°ï¼Œç”¨åˆ°çš„å‡½æ•°å¦‚ä¸‹

```python
def linreg(X,w,b):
"""å»ºç«‹æ¨¡å‹"""
    return torch.matmul(X, w) + b
```

```python
def square_loss(y_hat,y):
"""æŸå¤±å‡½æ•°"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

#### æ•°æ®è¿­ä»£å™¨

Â Â Â Â å®šä¹‰ä¸€ä¸ªæ•°æ®è¿­ä»£å™¨å®ç°éšæœºåˆ†æ‰¹æ¬¡ï¼Œ*yield*å…³é”®å­—çš„ä½œç”¨ï¼šç”¨äºå®šä¹‰ç”Ÿæˆå™¨å‡½æ•°ã€‚ç”Ÿæˆå™¨å‡½æ•°ä¸æ™®é€šå‡½æ•°ä¸åŒï¼Œå®ƒçš„æ‰§è¡Œæ˜¯å»¶è¿Ÿçš„ï¼Œåªæœ‰åœ¨éœ€è¦æ—¶æ‰ä¼šäº§ç”Ÿä¸€ä¸ªå€¼ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¯¹æ¯”

```python
def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices [i : min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]
```

Â Â Â Â ä¸¤è€…çš„ä¸åŒå°†åœ¨è°ƒç”¨æ—¶ä½“ç°

```python
def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices [i : min(i+batch_size,num_examples)])
        return features[batch_indices],labels[batch_indices]
#è°ƒç”¨å¿…é¡»æœ‰ç”³è¯·ç©ºé—´å…¨éƒ¨å­˜ä¸‹
a = data_iter(batch_size,features,labels)
```

#### å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

```python
def sgd(params,lr,batch_size):
    """å°æ‰¹é‡æ¢¯åº¦ä¸‹é™"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

#### è®­ç»ƒ

```python
"""åˆå§‹åŒ–"""
batch_size = 10
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

num_epoch = 5
lr = 0.03
net = linreg
loss = square_loss
for epoch in range(num_epoch):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

Â Â Â Â `for X, y in data_iter(batch_size, features, labels):`è¿™é‡Œä½“ç°å‡ºäº†ç”Ÿæˆå™¨å‡½æ•°çš„ä½œç”¨ï¼Œå¦‚æœä½ ä¸ä½¿ç”¨ `yield`ï¼Œè€Œæ˜¯åœ¨å‡½æ•°å†…éƒ¨ç›´æ¥è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¹æ¬¡æ•°æ®çš„æ•°æ®ç»“æ„ï¼Œæ¯”å¦‚åˆ—è¡¨ï¼Œé‚£ä¹ˆä½ éœ€è¦ä¸€æ¬¡æ€§å°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜ä¸­ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´å†…å­˜ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ã€‚

Â Â Â Â `with torch.no_grad()`ç”¨äºæŒ‡å®šä¸€æ®µä»£ç å—ï¼Œåœ¨è¿™ä¸ªä»£ç å—ä¸­ï¼ŒPyTorchä¼šå…³é—­æ¢¯åº¦è®¡ç®—ã€‚å› ä¸ºwï¼Œbåœ¨åˆ›å»ºæ—¶`requires_grad = True`åœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼ŒPyTorchä¼šè‡ªåŠ¨è®¡ç®—è¿™äº›æ¢¯åº¦ï¼Œåœ¨è¿›è¡Œæ¨¡å‹å‚æ•°æ›´æ–°çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸å†éœ€è¦ç»§ç»­ä¿ç•™ä¹‹å‰è®¡ç®—çš„æ¢¯åº¦ï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒä½¿ç”¨å½“å‰æ¢¯åº¦è¿›è¡Œå‚æ•°æ›´æ–°ã€‚ä¸ºäº†å‡å°‘å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å¼€é”€ï¼Œé€šå¸¸ä¼šåœ¨è¿›è¡Œå‚æ•°æ›´æ–°æ—¶æ¸…é›¶ä¹‹å‰è®¡ç®—çš„æ¢¯åº¦ï¼Œä»¥é˜²æ­¢æ¢¯åº¦ä¿¡æ¯ç´¯ç§¯ã€‚ä¹Ÿå°±æ˜¯sgdå‡½æ•°ä¸­çš„`param.grad.zero_()`

## å®Œæ•´ä»£ç ä»¥åŠç»“æœ

```python
from matplotlib import pyplot as plt
import random
import torch as tc
import numpy as np
# y = wx + b
num_inputs = 2
num_examples = 1000
true_w = [2,-3.4]
true_b = 4.2
features = tc.randn((num_examples,num_inputs))
labels = true_w[0] * features[:,0] + true_w[1] * features[:,1] + true_b
labels += tc.normal(mean=0, std=0.01,size=labels.shape)

plt.scatter(features[:,0].numpy(),labels.numpy(),s=1)
plt.xlabel("Feature 1")
plt.ylabel("Labels")
plt.title("Scatter Plot")
plt.scatter(features[:,1].numpy(),labels.numpy(),edgecolors='r',s=1)
plt.legend

def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = tc.tensor(indices [i : min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]




def square_loss(y_hat,y):
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

def linreg(X,w,b):
    return tc.matmul(X, w) + b

def sgd(params,lr,batch_size):
    """å°æ‰¹é‡æ¢¯åº¦ä¸‹é™"""
    with tc.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()



batch_size = 10
w = tc.normal(0, 0.01, size=(2,1), requires_grad=True)
b = tc.zeros(1, requires_grad=True)

num_epoch = 5
lr = 0.03
net = linreg
loss = square_loss
ind = 0
for epoch in range(num_epoch):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with tc.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')



x_val = np.arange(features[:, 1].min(), features[:, 1].max(),0.1)

w_val = w[1].detach().numpy().item()
b_val = b.detach().numpy()
y_val = w_val * x_val + b_val
print(y_val)

plt.plot(x_val, y_val)
plt.legend
plt.show()
```

ä»£ç ä¸­éœ€è¦æ³¨æ„çš„æ˜¯torchçš„ç”Ÿæˆè‡ªå®šä¹‰æ ‡å‡†åˆ†å¸ƒéšæœºæ•°

`torch.randn` å‡½æ•°ç”¨äºç”Ÿæˆæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œè€Œä¸å…è®¸ç›´æ¥è®¾ç½®æ ‡å‡†å·®ã€‚å¦‚æœä½ æƒ³ç”Ÿæˆæœä»å…¶ä»–æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œå¯ä»¥ä½¿ç”¨ `torch.normal` å‡½æ•°ã€‚

`torch.normal(mean=0, std=0.01,size=labels.shape)` å‡å€¼0 ï¼Œæ ‡å‡†å·®0.01ï¼Œå½¢çŠ¶ä¸ºlabelsçš„å½¢çŠ¶(1ï¼Œ1000)

`torch.randn(size=(2,1))`ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„å½¢çŠ¶ä¸º(2ï¼Œ1)çš„éšæœºæ•° 

ç”¨`normal`å¿…é¡»æŒ‡å®š`mean std size`çš„å‚æ•°ï¼Œç”¨`randn`ä¸èƒ½ä¿®æ”¹`mean std`

ç»“æœå¦‚ä¸‹ï¼Œå…¶é¢„æµ‹çš„æ˜¯$w_2$çš„æƒé‡ä»¥åŠåç§»$b$

<img title="" src="images/2024-01-26-20-06-57-image.png" alt="" width="635">

# Softmaxå›å½’

softmaxå›å½’è™½ç„¶æ˜¯å›å½’ï¼Œå®é™…ä¸Šæ˜¯ä¸ªåˆ†ç±»é—®é¢˜

Â Â Â Â Â ç®€å•ç†è§£ï¼šç»™æ¯ä¸ªå›å½’å¾—å‡ºçš„outputæ‰“åˆ†æ¯”å¦‚[10ï¼Œ1ï¼Œ1] ç„¶åå°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œ~~ï¼ˆè¿™é‡Œæ²¡æœ‰ç”¨softmaxçš„è½¬åŒ–æ–¹å¼ï¼‰~~ è½¬æ¢[0.833,0.083,0.083] æ¦‚ç‡ç›¸åŠ ç»“æœä¸º1ï¼Œä¸”ç¬¬ä¸€ä¸ªæ¦‚ç‡è¿œå¤§äºåä¸¤ä¸ªï¼Œåˆ™åˆ†ç±»ä¸ºç¬¬ä¸€ç±»ã€‚

### åˆ†ç±»é—®é¢˜

Â Â Â Â Â Â Â 1.ç›´è§‚åšæ³•ï¼š

æœ€ç›´æ¥çš„æƒ³æ³•æ˜¯é€‰æ‹©$y=\{1,2,3\}$ï¼Œå…¶ä¸­æ•´æ•°åˆ†åˆ«ä»£è¡¨ç‹—çŒ«é¸¡ã€‚
è¿™æ˜¯åœ¨è®¡ç®—æœºä¸Šå­˜å‚¨æ­¤ç±»ä¿¡æ¯çš„æœ‰æ•ˆæ–¹æ³•ã€‚ å¦‚æœç±»åˆ«é—´æœ‰ä¸€äº›è‡ªç„¶é¡ºåºï¼Œ
æ¯”å¦‚è¯´æˆ‘ä»¬è¯•å›¾é¢„æµ‹$\{å©´å„¿,å„¿ç«¥,é’å°‘å¹´,é’å¹´äºº,ä¸­å¹´äºº,è€å¹´äºº\}$ï¼Œ
é‚£ä¹ˆå°†è¿™ä¸ªé—®é¢˜è½¬å˜ä¸ºå›å½’é—®é¢˜ï¼Œå¹¶ä¸”ä¿ç•™è¿™ç§æ ¼å¼æ˜¯æœ‰æ„ä¹‰çš„ã€‚

<mark>ä½†æ˜¯å¤§å¤šæ•°åˆ†ç±»é—®é¢˜éƒ½ä¸ä¼šæ˜¯è¿™æ ·</mark>

Â Â Â Â Â Â Â Â 2.Â *ç‹¬çƒ­ç¼–ç *ï¼ˆone-hot encodingï¼‰ï¼š

æ¯”å¦‚ä¸€ä¸ªå‘é‡$[x_1,x_2,x_3]$åˆ†åˆ«ä»£è¡¨(çŒ« ï¼Œç‹— ï¼Œ é¸¡)   $[1,0,0]$ä»£è¡¨ğŸ±ï¼Œ$[0,1,0]$ä»£è¡¨ğŸ¶ï¼Œ$[0,0,1]$ä»£è¡¨ğŸ¥

Â Â Â Â Â Â Â Â æ‰€ä»¥è¾“å‡ºä¸º$y=\{(1,0,0),(0,1,0),(0,0,1)\}$

### ç¥ç»ç½‘ç»œ

Â Â Â Â Â Â Â Â è¿˜æ˜¯ç”¨çŒ«ç‹—é¸¡åˆ†ç±»æ¥è¯´ï¼Œå‡è®¾çŒ«ç‹—é¸¡åˆ†ç±»æœ‰å‘é‡$\mathbf{x}=[x_1,x_2,x_3,x_4]$ä½œä¸º`ç‰¹å¾å‘é‡`

<mark>ä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬$X$æœ‰4ä¸ªç‰¹å¾</mark>

Â Â Â Â Â Â Â  æœ‰æƒé‡çŸ©é˜µ$W$,åç½®å‘é‡$\mathbf{b}$

$$
W=\begin{bmatrix}
  \beta_1\\ 
 \beta_2\\
 \beta_3\\
\end{bmatrix}

=
\begin{bmatrix}
  w_{11} & w_{12} & w_{13} & w_{14}\\
  w_{21} & w_{22} & w_{23} & w_{24}\\
  w_{31} & w_{32} & w_{33} & w_{34}\\
\end{bmatrix},
b=\begin{bmatrix}
  b_1\\ 
 b_2\\
 b_3\\
\end{bmatrix}
$$

Â Â Â Â Â Â Â Â Â å¯¹äºè¾“å‡º$y$æœ‰$o_1 ä»£è¡¨ğŸ±,o_2ä»£è¡¨ğŸ¶,o_3ä»£è¡¨ğŸ¥$

Â Â Â Â Â Â Â  Â å¯¹äºæƒé‡å’Œåç½®$\beta_1æ˜¯ğŸ±çš„æƒé‡ï¼Œb_1æ˜¯ğŸ±çš„åç½®$

$$
o_1=\mathbf{x}\beta_1^T+b_1 = x_1*w_{11}+x_2*w_{12}+x_3*w_{13}+x_4*w_{14} +b_1\\
o_2=\mathbf{x}\beta_2^T+b_2 = x_1*w_{21}+x_2*w_{22}+x_3*w_{23}+x_4*w_{24}   +b_2\\
o_3=\mathbf{x}\beta_3^T+b_3= x_1*w_{31}+x_2*w_{32}+x_3*w_{33}+x_4*w_{34}+b_3
$$

Â Â Â Â Â Â Â Â Â è¿™å…¶å®æ˜¯ä¸‰æ¬¡çº¿æ€§å›å½’é—®é¢˜ã€‚çº¿æ€§å›å½’æ˜¯å•å±‚ç¥ç»ç½‘ç»œï¼Œè€Œsoftmaxç¥ç»ç½‘ç»œä¹Ÿæ˜¯å•å±‚çš„ç¥ç»ç½‘ç»œã€‚

> Â Â Â Â Â Â Â Â <img src="images\2024-01-27-15-25-38-image.png" title="" alt=" " width="522">

Â Â Â Â Â Â Â Â é‚£ä¹ˆæˆ‘ä»¬å¯¹å½“å‰è¿™ä¸ªæ ·æœ¬åˆ†åˆ«è®¡ç®—å‡º$o_1,o_2,o_3$çš„å€¼ï¼Œç±»ä¼¼äºæ‰“åˆ†ï¼Œå¯ä»¥ä¸ºå½“å‰æ ·æœ¬å¾—å‡ºåœ¨åƒğŸ±ï¼ŒğŸ¶ï¼ŒğŸ¥æ–¹é¢çš„ä¸‰ä¸ªä¸åŒçš„åˆ†æ•°ã€‚

### Softmax

Â Â Â Â Â Â Â Â è·å¾—äº†ğŸ±ğŸ¶ğŸ¥çš„åˆ†æ•°ç†è®ºä¸Šå·²ç»å¯ä»¥åˆ†ç±»å‡ºæ¥äº†ï¼Œåªéœ€è¦æ¯”å¦‚ğŸ±çš„åˆ†æ•°è¿œå¤§äºğŸ¶ğŸ¥çš„åˆ†æ•°ï¼Œå°±å¯ä»¥å°†å…¶åˆ†ç±»ä¸ºğŸ±ã€‚

Â Â Â Â Â Â Â Â ä½†æ˜¯ä»¥ä¸‹æ˜¯ç›´æ¥åˆ†ç±»çš„ç¼ºç‚¹ï¼š(å‡ºè‡ªchatGpt)

> 1. **èŒƒå›´ä¸ä¸€è‡´ï¼š** è¾“å‡ºå€¼çš„èŒƒå›´å¯èƒ½ä¸ä¸€è‡´ï¼Œä¸åŒç±»åˆ«çš„è¾“å‡ºå¯èƒ½å¤„äºä¸åŒçš„æ•°å€¼å°ºåº¦ä¸Šï¼Œä½¿å¾—éš¾ä»¥æ¯”è¾ƒå®ƒä»¬çš„é‡è¦æ€§æˆ–ç½®ä¿¡åº¦ã€‚
> 
> 2. **æ— æ³•è§£é‡Šä¸ºæ¦‚ç‡ï¼š** ç›´æ¥çš„è¾“å‡ºå€¼æœªç»è¿‡å½’ä¸€åŒ–ï¼Œå› æ­¤ä¸èƒ½è¢«è§£é‡Šä¸ºæ¦‚ç‡ã€‚æ¦‚ç‡åº”è¯¥åœ¨0åˆ°1ä¹‹é—´ï¼Œå¹¶ä¸”æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ä¹‹å’Œåº”ä¸º1ã€‚ç›´æ¥æ¯”è¾ƒè¾“å‡ºå€¼æ— æ³•æä¾›è¿™ç§æ¦‚ç‡è§£é‡Šã€‚
> 
> 3. **æ•°å€¼ä¸ç¨³å®šæ€§ï¼š** ç›´æ¥æ¯”è¾ƒåŸå§‹è¾“å‡ºå€¼å¯èƒ½å—åˆ°æ•°å€¼ä¸ç¨³å®šæ€§çš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¯èƒ½ä¼šå‡ºç°æ•°å€¼æº¢å‡ºæˆ–ä¸‹æº¢çš„é—®é¢˜ã€‚Softmaxé€šè¿‡æŒ‡æ•°è¿ç®—å’Œå½’ä¸€åŒ–è¿‡ç¨‹ï¼Œæœ‰åŠ©äºå¤„ç†è¿™äº›æ•°å€¼ç¨³å®šæ€§çš„é—®é¢˜ã€‚
> 
> 4. **æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š** åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç›´æ¥æ¯”è¾ƒåŸå§‹è¾“å‡ºå€¼å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œè¿™åœ¨åå‘ä¼ æ’­ä¸­å¯èƒ½å½±å“æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§ã€‚
> 
> 5. **ä¸é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼š** ç›´æ¥æ¯”è¾ƒè¾“å‡ºå€¼å¯¹äºå¤šåˆ†ç±»é—®é¢˜è€Œè¨€ï¼Œå¯èƒ½æ— æ³•æä¾›ä¸€ä¸ªæ¸…æ™°çš„å†³ç­–æ ‡å‡†ã€‚Softmaxé€šè¿‡å°†è¾“å‡ºæ˜ å°„åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæœ‰åŠ©äºåœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­åšå‡ºå†³ç­–ã€‚

Â Â Â Â Â Â Â Â ç®€è€Œè¨€ä¹‹ï¼Œå°†åŸå§‹è¾“å‡ºå€¼è½¬åŒ–ä¸ºæ¦‚ç‡æ›´åŠ æœ‰åˆ©ã€‚

#### Softmaxè¿ç®—

<div>
<span style="font-size:larger;">
</span>

</div>

$$
\hat{y_i}=Softmax(o_i)=\frac{e^{o_i}}{\sum_{k=1}^n{e^{o_k}}}
$$

Â Â Â Â Â Â Â Â ç°åœ¨æˆ‘ä»¬æŠŠ$\mathbf{o}$è½¬åŒ–ä¸ºäº†$\mathbf{\hat{y}}$ ,è€Œä¸”å¯¹äºä»»æ„$\mathbf{\hat{y}}$,æœ‰$0\leq\mathbf{\hat{y}}\leq 1$

Â Â Â Â Â Â Â Â è€Œä¸”$\sum_i{\hat{y_i}} = 1$ï¼Œå¯ä»¥è§†ä¸ºä¸€ä¸ªæ­£ç¡®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç»è¿‡è¯¥è¿ç®—ä¸ä¼šæ”¹å˜å„ä¸ªè¾“å‡ºä¹‹é—´çš„å¤§å°å…³ç³»ã€‚å› æ­¤æˆ‘ä»¬ä¾ç„¶å¯ä»¥è®¤ä¸º$max(\hat{y_i})$ä¸ºåˆ†ç±»ç»“æœã€‚

### æŸå¤±å‡½æ•°

Â Â Â Â Â Â Â Â æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥åº¦é‡é¢„æµ‹çš„æ•ˆæœã€‚æˆ‘ä»¬å°†ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡

~~å…¶å®çº¿æ€§å›å½’ç”¨çš„ä¹Ÿæ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡~~

> <mark>ä¸ºäº†é˜²æ­¢å¿˜è®°æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ­¥éª¤ï¼š</mark>
> 
> è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ›´ç®€å•çš„ä¾‹å­ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¡¬å¸ï¼Œæˆ‘ä»¬æƒ³è¦ä¼°è®¡è¿™ä¸ªç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ pã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸‰æ¬¡æŠ•æ·ï¼Œç»“æœåˆ†åˆ«æ˜¯æ­£é¢ã€åé¢ã€æ­£é¢ã€‚
> 
> æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼°è®¡ç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ pã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªä¼¼ç„¶å‡½æ•°ï¼Œè¡¨ç¤ºè§‚æµ‹åˆ°è¿™ä¸‰æ¬¡ç»“æœçš„æ¦‚ç‡ï¼š
> 
> P(è§‚æµ‹æ•°æ®âˆ£p)=pâ‹…(1âˆ’p)â‹…p
> 
> è¿™é‡Œï¼Œp è¡¨ç¤ºæ­£é¢æœä¸Šçš„æ¦‚ç‡ï¼Œè€Œ 1âˆ’p è¡¨ç¤ºåé¢æœä¸Šçš„æ¦‚ç‡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿å¾—è§‚æµ‹æ•°æ®çš„ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„ pã€‚
> 
> å–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼ˆå¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æœ€å¤§åŒ–ç­‰æ•ˆäºä¼¼ç„¶å‡½æ•°çš„æœ€å¤§åŒ–ï¼‰ï¼š
> 
> logP(è§‚æµ‹æ•°æ®âˆ£p)=log(p)+log(1âˆ’p)+log(p)
> 
> ç°åœ¨ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªå¯¹æ•°ä¼¼ç„¶å‡½æ•°å…³äº p æ±‚åå¯¼æ•°ï¼Œå¹¶ä»¤å…¶ç­‰äºé›¶ï¼Œè§£å‡º p çš„å€¼ã€‚åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªæ–¹ç¨‹å¾—åˆ°æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç»“æœã€‚
> 
> dpdâ€‹logP(è§‚æµ‹æ•°æ®âˆ£p)=p1â€‹âˆ’1âˆ’p1â€‹+p1â€‹=0
> 
> è§£è¿™ä¸ªæ–¹ç¨‹ï¼Œæˆ‘ä»¬å¾—åˆ° p=$\frac{2}{3}$ã€‚æ‰€ä»¥ï¼Œæ ¹æ®è§‚æµ‹æ•°æ®ï¼Œç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸º $\frac{2}{3}$â€‹ã€‚

å…ˆæ¨å¯¼ä¸€ä¸‹ï¼š

Â Â Â Â Â Â Â Â æˆ‘ä»¬å·²ç»ç”¨softmaxè·å¾—äº†ä¸€ä¸ªå‘é‡$\mathbf{\hat{y}}$ï¼Œå¯ä»¥å°†å…¶è§†å¯¹ç»™å®šä»»æ„è¾“å…¥$\mathbf{x}$çš„æ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡â€ã€‚æ¯”å¦‚$\hat{y_1}=P\{y=ğŸ±|\bf{x}\}$

Â Â Â Â Â Â Â Â é¦–å…ˆæˆ‘ä»¬æ¥æ±‚ä¼¼ç„¶å‡½æ•° ä¸º  $P(Y|X)=\prod_{i=1}^{n} P(\hat{y}^{(i)}|x^{(i)})$

Â Â Â Â Â Â Â Â å› ä¸ºè¿ä¹˜ä¼šå¯¼è‡´ä¼¼ç„¶å‡½æ•°å€¼è¿‡å°ï¼Œç”¨å¯¹æ•°åŒ–è¿ä¹˜ä¸ºæ±‚å’Œï¼ŒåŠ è´Ÿå·å³å¯é€šè¿‡æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹å¼è·å¾—æœ€å°å€¼ï¼Œå³ï¼š

$$
logP(Y|X)=\sum_{i=1}^{n} logP(\mathbf{\hat{y}}^{(i)}|\mathbf{x}^{(i)})  \\
$$

$$
-logP(Y|X)=\sum_{i=1}^{n} -logP(\mathbf{\hat{y}}^{(i)}|\mathbf{x}^{(i)}) =l(\bf{y},\bf{\hat{y}})

$$

åŒºåˆ«äºçº¿æ€§å›å½’ï¼Œsoftmaxæ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¦‚æœè¦å°†æ ·æœ¬åˆ†ç±»ä¸ºğŸ±ï¼Œæˆ‘ä»¬éœ€è¦ğŸ±çš„æ¦‚ç‡å¤§äºğŸ¶å’ŒğŸ¥ï¼Œç”¨çº¿æ€§å›å½’çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹å·®æ¥ä½œä¸ºæŸå¤±å‡½æ•°æ˜æ˜¾æ˜¯ä¸åˆé€‚çš„ã€‚

Â Â Â Â Â Â Â Â äºæ˜¯æˆ‘ä»¬å¼•å…¥`äº¤å‰ç†µæŸå¤±å‡½æ•°`

$$
l(\mathbf{y},\mathbf{\hat{y}})=H(y^{(i)},\hat{y}^{(i)})=\sum_{j=1}^qy_jlog\hat{y_j}
$$

è®¾$y_i$æŒ‡çš„æ˜¯çœŸå®å€¼çš„æ¦‚ç‡ï¼Œå‡å¦‚å·²çŸ¥ğŸ±ï¼Œä¸ºğŸ±çš„æ¦‚ç‡ä¸º1ï¼Œä¸ºğŸ¶ğŸ¥çš„æ¦‚ç‡ä¸º0

è®¾æ€»å…±æœ‰qä¸ªè¾“å‡ºï¼Œç”±$y_i=1$åªèƒ½æœ‰ä¸€ä¸ªï¼Œå…¶ä½™éƒ½ä¸º0

$$
H(y^{(i)},\hat{y}^{(i)})=-log\hat{y}_{y^{(i)}}^{(i)}
$$

> è§£é‡Šå½“$åªæœ‰å½“y_j=1æ—¶ï¼Œä¹Ÿå°±æ˜¯y_j=y_iæ—¶H\neq0ï¼Œä¹Ÿå°±æ˜¯log\hat{y}^{(i)}$
> 
> å‡å¦‚æœ‰ç‹¬çƒ­ç¼–ç  [0,1,0],æ¨¡å‹åœ¨æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­é¦–æ¬¡ç»™å‡ºçš„æ¦‚ç‡æ˜¯[0.6ï¼Œ0.1ï¼Œ0.3]ï¼Œç”±äºçœŸå®çš„åˆ†ç±»ä¸ºç¬¬äºŒç±»ï¼Œæ‰€ä»¥å°±ç®—æ¨¡å‹ç»™å‡ºç¬¬ä¸€ç±»çš„æ¦‚ç‡æœ€å¤§ï¼Œä¾ç„¶ä¼šé€‰æ‹©ç¬¬äºŒä¸ªæ¦‚ç‡åŠ å…¥æŸå¤±å‡½æ•°çš„è®¡ç®—ã€‚è¿™æ ·ä¼šé¼“åŠ±ç¬¬äºŒç±»çš„è¾“å‡ºæ¦‚ç‡æ›´æ¥è¿‘çœŸå®æ ‡ç­¾ã€‚
> 
> åŠ è´Ÿå·æ˜¯å› ä¸ºåœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œé€šå¸¸æ˜¯æœ€å°åŒ–æŸå¤±ã€‚åŠ è´Ÿå·å³å¯é€šè¿‡æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹å¼è·å¾—æœ€å°å€¼

## ä¼˜åŒ–ç®—æ³•

Â Â Â Â ç”±äºåœ¨å•ä¸ªè¾“å‡ºçš„è®¡ç®—æ–¹é¢ä¸çº¿æ€§å›å½’ç±»ä¼¼ï¼Œä¾ç„¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ–¹æ³•

Â Â Â Â Â å› æ­¤è¦å¯¹æŸå¤±å‡½æ•°æ±‚å¯¼ï¼š

$$
\begin{align*}
l(\mathbf{y}, \mathbf{\hat{y}}) &= -\sum_{j=1}^{q} y_j \log\left(\frac{\exp(o_j)}{\sum_{k=1}^{q} \exp(o_k)}\right) \\
&= -\sum_{j=1}^{q} y_j \log\left(\frac{\exp(o_j)}{\sum_{k=1}^{q} \exp(o_k)}\right) - \sum_{j=1}^{q} y_j o_j \\
&= \log\left(\sum_{k=1}^{q} \exp(o_k)\right) - \sum_{j=1}^{q} y_j o_j.
\end{align*}
$$

## è¯»å…¥æ•°æ®é›†

```python
  def get_fashion_mnist_labels(labels):
    """return text labels of Fashion-MNIST dataset """
    text_labels = [
        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',
        'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```

`return [text_labels[int(i)] for i in labels]`æ˜¯ä¸€ä¸ªåˆ—è¡¨æ¨å¯¼å¼ï¼Œå®ƒå°†è¾“å…¥çš„ `labels` åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´  `i` è½¬æ¢ä¸º `text_labels[int(i)]`ã€‚æœ€ç»ˆè¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰è½¬æ¢åå…ƒç´ çš„æ–°åˆ—è¡¨ã€‚

```python
def show_images(images, num_rows, num_cols, titles=None, scale=1.5):  
    figsize = (num_cols * scale, num_rows * scale)  
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)  
    axes = axes.flatten()  
    for i, (ax, img) in enumerate(zip(axes, images)):  
        ax.imshow(img,)  
        ax.axes.get_xaxis().set_visible(False)  
        ax.axes.get_yaxis().set_visible(False)  
        if titles:  
            ax.set_title(titles[i])  
    plt.show()
```

è¿™æ˜¯æ˜¾ç¤ºå›¾ç‰‡

```python
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
```

pytorchä¸­å›¾ç‰‡ç»å¸¸æ˜¯æœ‰ä¸‰ä¸ªç»´åº¦`(channels, height, width)`è¿™é‡Œchanelsæ²¡ç”¨ æ¢æˆäº†`batch_size`å¯ä»¥åœ¨`show_image`å‡½æ•°ä¸­ä¸€æ¬¡æ€§è¯»å…¥18å¼ å›¾ç‰‡ã€‚å¯ä»¥é€šè¿‡éå†`channels`

å±•ç¤ºæ‰€æœ‰å›¾ç‰‡ã€‚

```python
def get_dataloader_workers():
    return 4
train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, 
                              num_workers=get_dataloader_workers())
timer = d2l.Timer()
for X, y in train_iter:
    continue
print(f'{timer.stop():.2f} sec')
```

æ•°æ®è¿­ä»£å™¨ï¼Œå¹¶è®°å½•è¯»å–æ—¶é—´ï¼Œ`get_dataloader_workers`è®¾ç½®å¼€å¤šå°‘è¿›ç¨‹åŠ è½½æ•°æ®

## å…³é”®å‡½æ•°

### Softmaxè¿ç®—

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1,keepdim=True)
    return X_exp / partition
```

`partition = X_exp.sum(1,keepdim=True)`è¡¨ç¤ºæŒ‰ç…§æ¨ªå‘æ±‚å’Œï¼Œ~~ï¼ˆæ±‚è¡Œå’Œï¼‰~~

åšä¸ªå®éªŒï¼šæœ€ä¸Šé¢ä¸ºXï¼Œè‡ªä¸Šè€Œä¸‹åˆ†åˆ«æ˜¯`X.sum(0, keepdim=True)`

 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â `X.sum(1, keepdim=True)`

> ![](images\2024-01-27-21-31-30-image.png)
> 
> ä¸ºä»€ä¹ˆæ˜¯æ¨ªå‘ï¼Ÿ
> 
> å› ä¸ºè°ƒç”¨softmaxçš„æ˜¯æ¨¡å‹é¢„æµ‹å€¼ä¹Ÿå°±æ˜¯$\hat{\mathbf{y}}={\mathbf{O}}$è¿™é‡Œå‡è®¾nä¸ºæ ·æœ¬æ•°
> 
> $$
> \mathbf{O} = \mathbf{X}\mathbf{W}^T+\mathbf{B} = \begin{bmatrix}
     \mathbf{x_1W^T}+\mathbf{b_1}\\
     \mathbf{x_2W^T}+\mathbf{b_2}\\
...\\
     \mathbf{x_nW^T}+\mathbf{b_n}
                                            \end{bmatrix}
> $$
> 
> æ‰€ä»¥æœ€ç»ˆç»™å‡ºæ¨ªå‘æ˜¯åŒä¸€ä¸ªæ ·æœ¬ å¯¹äºä¸åŒæ ‡ç­¾çš„æ‰“åˆ†ã€‚
> 
> ![](images\2024-01-27-22-01-59-image.png)
> 
> éªŒè¯ä¸€ä¸‹ï¼šç”Ÿæˆä¸€ä¸ªäºŒè¡Œäº”åˆ—æ­£æ€åˆ†å¸ƒéšæœºçŸ©é˜µaï¼Œå°†å…¶è¿›è¡Œsoftmaxè¿ç®—ç»“æœå¦‚ä¸‹ï¼š
> 
> ![](images\2024-01-27-22-03-00-image.png)
> 
> æˆåŠŸè½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œä¸”æ¯è¡Œæ¦‚ç‡ä¹‹å’Œä¸º1ï¼Œæ˜¯æ­£ç¡®çš„

### Softmaxå›å½’æ¨¡å‹

```python
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

> è¿™é‡Œæ ¹æ®å…¬å¼å»ºç«‹èµ·äº†softmaxæ¨¡å‹ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯`X.reshape((-1,W.shape[0]))`è¿™é‡Œåº”è¯¥å°†X `reshape`æˆ`(batch_size,W.shape[0])`ï¼Œè¡Œå†™æˆ-1ä¼šè‡ªåŠ¨æ±‚è¯¥å¤„çš„å€¼~~å·æ‡’~~ã€‚
> 
> ç›´æ¥`+b`æ˜¯åˆ©ç”¨äº†å¹¿æ’­æœºåˆ¶ï¼Œ`b`è‡ªåŠ¨è¡¥é½ä¸ºç›¸åŒshapeçš„çŸ©é˜µã€‚

### äº¤å‰ç†µæŸå¤±

```python
y = torch.tensor([0, 2])  
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])  
print(y_hat)  
print(y_hat[[0,1], y])
```

> å…ˆçœ‹ä¸Šé¢è¿™ä¸²ä»£ç ğŸ‘†ï¼Œè¾“å‡ºç»“æœğŸ‘‡
> 
> ![](images\2024-01-27-23-18-32-image.png)
> 
> Â Â Â Â å‡è®¾ä¸€ç»´æ•°ç»„`y`æ˜¯æ ·æœ¬çœŸå®æ‰€å±åˆ†ç±»çš„ç´¢å¼•ï¼Œè½¬åŒ–ä¸ºğŸ±ğŸ¶ğŸ¥è¯­è¨€ï¼Œå°±ä»£è¡¨ç€ğŸ±ï¼ŒğŸ¥æ‰€åœ¨çš„ä½ç½®ã€‚`y_hat`æ˜¯æ¨¡å‹ç»™å‡ºçš„ä¸¤ä¸ªé¢„æµ‹æ¦‚ç‡ã€‚æ ¹æ®ä¹‹å‰çš„æ¨å¯¼ï¼Œäº¤å‰ç†µæŸå¤±è¦æ±‚æˆ‘ä»¬å¦‚æœçœŸå®çš„æ˜¯ğŸ±ï¼Œå°±ç®—æ¨¡å‹ç»™å‡ºğŸ±çš„æ¦‚ç‡æœ€å°ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ä¹Ÿè¿˜æ˜¯é€‰å–ä½œä¸ºğŸ±çš„æ¦‚ç‡ã€‚è¿™é‡Œè¡¨ç°ä¸ºçœŸå®çš„`y`ç´¢å¼•ä¸º`[0,2]`æ‰€ä»¥é€‰æ‹©ä½œä¸º`[0,2]`çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯`[0.1000, 0.5000]`ã€‚
> 
> Â Â Â Â è‡³äºä»£ç ä¸ºä»€ä¹ˆæ˜¯`y_hat[[0,1], y]`ï¼Œè¿™æ˜¯èŠ±å¼ç´¢å¼•`(fancy indexing)`,è¡¨ç¤ºä» `y_hat` ä¸­é€‰æ‹©ç¬¬ 0 è¡Œå’Œç¬¬ 1 è¡Œï¼ˆå¯¹åº”ä¸¤ä¸ªæ ·æœ¬ï¼‰,ç„¶ååˆ†åˆ«åœ¨ç¬¬0è¡Œçš„ç¬¬0ä¸ªé€‰å–å’Œç¬¬1è¡Œç¬¬äºŒä¸ªé€‰å–ã€‚

```python
def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])
```

ğŸ‘†è¿™å°±æ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°`range(len(y_hat))`æŒ‡çš„æ˜¯æ‰€æœ‰çš„æ ·æœ¬çš„ç´¢å¼•ï¼Œ`y`æŒ‡çš„æ˜¯å½“å‰æ ·æœ¬çœŸå®æ‰€å¤„çš„åˆ†ç±»çš„ç´¢å¼•ã€‚

> å°†ä¸Šé¢çš„è°ƒç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°å¾—åˆ°ğŸ‘‡ç»“æœ
> 
> ![](images\2024-01-27-23-44-58-image.png)
> 
> ç”±å–ç›¸åå¯¹æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°æ¦‚ç‡è¶Šå°æŸå¤±è¶Šå¤§ï¼Œæ¦‚ç‡è¶Šå¤§æŸå¤±è¶Šå°ã€‚

Â Â Â Â æœ‰äº†æ¨¡å‹å’ŒæŸå¤±å‡½æ•°ï¼Œæ¥ä¸‹æ¥å°±æ˜¯ä¼˜åŒ–ç®—æ³•äº†

Â Â Â Â å¯¹äºæ¯ä¸ªè¾“å‡º$o$æ¥è¯´ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ã€‚

```python
lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)
```

Â Â Â Â å½“ç„¶è¿™é‡Œçš„ä¼˜åŒ–ç®—æ³•æ˜¯å¯¹äºå„ä¸ªè¾“å‡ºçš„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦çŸ¥é“æ¨¡å‹é¢„æµ‹åˆ†ç±»çš„å‡†ç¡®åº¦ï¼š

å…·ä½“æ“ä½œèµ·æ¥ä¹Ÿå¾ˆç®€å•ï¼Œåªéœ€è¦æŠŠé¢„æµ‹æ­£ç¡®çš„æ•°é‡ç»Ÿè®¡å‡ºæ¥å³å¯ï¼š

```python
def accuracy(y_hat, y):  
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡ã€‚"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

> è§£é‡Šï¼š
> 
> Â Â Â Â Â Â `if len(y_hat.shape) > 1 and y_hat.shape[1] > 1` å¦‚æœ`y_hat`æ˜¯äºŒç»´çŸ©é˜µï¼Œè¡Œè¡¨ç¤ºæ ·æœ¬ï¼Œåˆ—è¡¨ç¤ºæ¦‚ç‡ã€‚
> 
> Â Â Â Â Â Â Â Â `y_hat = y_hat.argmax(axis=1)`è¿™é‡Œ`y_hat`è¢«èµ‹å€¼ä¸ºæ¨ªå‘æœ€å¤§å€¼çš„ç´¢å¼•ã€‚æ¯”å¦‚ï¼šy_hat = `[[0.1, 0.3, 0.6], `
> 
> Â Â Â Â Â Â Â Â               `[0.3, 0.2, 0.5]]`
> 
> `y_hat.argmax(axis=1)`è¿”å›`[2.,2.]` 
> 
>  Â Â Â Â Â Â Â Â `cmp = y_hat.type(y.dtype) == y` åˆ¤æ–­æ˜¯å¦ä¸`y`ä¹Ÿå°±æ˜¯çœŸå®çš„æ ‡ç­¾ç´¢å¼•ç›¸åŒï¼Œå®é™…ä¸Šæ˜¯ä¸¤ä¸ªå¼ é‡çš„æ¯ä¸ªå…ƒç´ ç›¸ä¸ã€‚ `cmp`ä¸º`Boolean`ç±»å‹ï¼Œ`y_hat`å¦‚æœä¸`y`å¯¹åº”ä½ç½®å…ƒç´ ç›¸ç­‰åˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`false`ï¼Œæ­£å¥½å¯¹åº”æ•°å€¼çš„`1`å’Œ`0`ï¼Œå¯ä»¥ç”¨æ¥ç»Ÿè®¡æ•°é‡ã€‚
> 
> å¦‚æœè¦å¾—åˆ°é¢„æµ‹æ­£ç¡®çš„æ¦‚ç‡åªéœ€è¦â—`y`çš„é•¿åº¦ï¼Œå³æ ·æœ¬æ•°
> 
> `accuracy(y_hat, y) / len(y)`ï¼Œç”¨ä¸Šè¿°ä¾‹å­ç»“æœåº”è¯¥ä¸º`0.5`

### è¯„ä¼°ä»»æ„æ¨¡å‹netçš„å‡†ç¡®ç‡

```python
def evaluate_accuracy(net, data_iter):  
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦ã€‚"""  
    if isinstance(net, torch.nn.Module):  
        net.eval()  
    metric = Accumulator(2)  
    for X, y in data_iter:  
        metric.add(accuracy(net(X), y), y.numel())  
    return metric[0] / metric[1]

class Accumulator:
    """åœ¨`n`ä¸ªå˜é‡ä¸Šç´¯åŠ ã€‚"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

> è§£é‡Šï¼š
> 
> Â Â Â  1. `isinstance` æ˜¯ Python ä¸­çš„ä¸€ä¸ªå†…ç½®å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥ä¸€ä¸ªå¯¹è±¡æ˜¯å¦å±äºæŒ‡å®šçš„ç±»æˆ–ç±»å‹ã€‚å®ƒçš„è¯­æ³•å¦‚ä¸‹ï¼š
> 
> `isinstance(object, classinfo)`
> 
> - `object`: è¦æ£€æŸ¥çš„å¯¹è±¡ã€‚
> - `classinfo`: å¯ä»¥æ˜¯ä¸€ä¸ªç±»å¯¹è±¡ã€ä¸€ä¸ªç±»å‹å¯¹è±¡æˆ–è€…ä¸€ä¸ªç”±ç±»å¯¹è±¡ç»„æˆçš„å…ƒç»„ï¼ˆç”¨äºæ£€æŸ¥å¯¹è±¡æ˜¯å¦å±äºå…¶ä¸­ä»»æ„ä¸€ä¸ªç±»ï¼‰ã€‚
> 
> `isinstance` è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ `object` æ˜¯ `classinfo` æŒ‡å®šçš„ç±»å‹ä¹‹ä¸€ï¼Œæˆ–è€…æ˜¯å…¶å­ç±»çš„å®ä¾‹ï¼Œåˆ™è¿”å› `True`ï¼›å¦åˆ™è¿”å› `False`ã€‚
> 
>   2.`net.eval()`è¿™è¡Œä»£ç å°†æ¨¡å‹ `net` è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œæ¨¡å‹é€šå¸¸ä¸ä¼šè¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œè¿™æœ‰åŠ©äºæé«˜è¯„ä¼°çš„é€Ÿåº¦å’Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚
> 
>   3.`metric = Accumulator(2)`è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªåä¸º `metric` çš„ç´¯åŠ å™¨ï¼ˆAccumulatorï¼‰ï¼Œè¯¥ç´¯åŠ å™¨ç”¨äºå­˜å‚¨ä¸¤ä¸ªå€¼ï¼Œåˆ†åˆ«æ˜¯æ­£ç¡®é¢„æµ‹çš„æ•°é‡å’Œæ€»é¢„æµ‹çš„æ•°é‡ã€‚Accumulator æ˜¯ä¸€ä¸ªç”¨äºç´¯åŠ æ•°å€¼çš„è‡ªå®šä¹‰ç±»ã€‚
> 
> Â Â Â Â Â Â Â Â æ¥çœ‹ä¸€ä¸‹`Accumulator`çš„æ„é€ å‡½æ•°
> 
> Â Â Â Â Â Â Â Â `def __init__(self, n):`
>         `        self.data = [0.0] * n`
> 
> Â Â Â Â Â Â Â Â pythonä¸­`[0.0] * n`or`n * [0.0]`ä¸æ˜¯ä¸€ä¸ª æ•°ç»„Ã—æ•°ï¼Œè€Œæ˜¯è¡¨ç¤ºåˆ›å»ºä¸€ä¸ªåŒ…å« n ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯ `0.0` çš„åˆ—è¡¨ã€‚åšä¸ªå®éªŒï¼š
> 
>    Â Â Â   ![ ](./images/2024-01-28-14-36-51-image.png)
> 
> Â Â Â Â Â Â Â Â ç»“æœğŸ‘‡
> 
> Â Â Â Â Â Â Â Â ![](./images/2024-01-28-14-37-19-image.png) 
> 
>   4.`metric.add(accuracy(net(X), y), y.numel())`: è¿™ä¸€è¡Œè®¡ç®—å¹¶ç´¯åŠ ä¸¤ä¸ªå€¼ã€‚`accuracy(net(X), y)` è®¡ç®—æ¨¡å‹åœ¨å½“å‰æ‰¹æ¬¡ä¸Šçš„å‡†ç¡®ç‡ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°ç´¯åŠ å™¨ä¸­çš„ç¬¬ä¸€ä¸ªä½ç½®ã€‚`y.numel()` åˆ™æ˜¯å½“å‰æ‰¹æ¬¡æ ·æœ¬çš„æ€»æ•°é‡ï¼Œå°†å…¶æ·»åŠ åˆ°ç´¯åŠ å™¨çš„ç¬¬äºŒä¸ªä½ç½®ã€‚
> 
> ```python
> def add(self, *args):
>     self.data = [a + float(b) for a, b in zip(self.data, args)]
> ```
> 
> Â Â Â Â `zip`æ˜¯ä¸€ä¸ªå†…å»ºå‡½æ•°ï¼Œå®ƒç”¨äºå°†å¤šä¸ªå¯è¿­ä»£å¯¹è±¡ï¼ˆä¾‹å¦‚åˆ—è¡¨ã€å…ƒç»„ç­‰ï¼‰ä¸­å¯¹åº”ä½ç½®çš„å…ƒç´ æ‰“åŒ…æˆä¸€ä¸ªå…ƒç»„ï¼Œç„¶åè¿”å›ç”±è¿™äº›å…ƒç»„ç»„æˆçš„è¿­ä»£å™¨ã€‚ç±»ä¼¼äºæ‹‰é“¾ï¼ŒæŠŠä¸¤ä¸ªåˆåœ¨ä¸€èµ·ã€‚
> 
> åšä¸ªå®éªŒï¼š
> 
> Â Â Â Â ![ ](images/2024-01-28-14-54-07-image.png)
> 
> ç»“æœğŸ‘‡
> 
> Â Â Â Â ![ ](images/2024-01-28-14-55-25-image.png)
> 
> ç¬¬0ä¸ªä½ç½®çš„å…ƒç´ è¢«ä¸ºä¸€ä¸ªå…ƒç»„ã€‚
> 
> Â Â Â Â ![ ](images/2024-01-28-14-59-33-image.png)
> 
> Â Â Â Â ![ ](images/2024-01-28-15-00-00-image.png)
> 
> Â Â Â Â å‡å¦‚ç¬¬ä¸€æ‰¹æœ‰2ä¸ªæ ·æœ¬5ç§åˆ†ç±»ï¼Œä¹Ÿå°±æ˜¯äºŒè¡Œäº”åˆ—çŸ©é˜µï¼Œ`y=[3, 0]`
> 
> $$
> \begin{bmatrix}
    0.1 & 0.2 &0.1 &0.5 &0.1 \\
    0.05 &0.15 &0.5 &0.15 &0.15
\end{bmatrix}
> $$
> 
> Â Â Â Â æ­¤æ—¶è®¡ç®—å‡†ç¡®åº¦åº”è¯¥ä¸º`0.5`ï¼Œ2ä¸ªé‡Œé¢å¯¹1ä¸ªï¼Œè°ƒç”¨`accuracy`å‡½æ•°ç»“æœåº”è¯¥ä¸º`1`ï¼Œç´¯åŠ å™¨æ•°æ®ä¸º`0`ï¼Œæ­¤æ—¶`zip`åˆæˆä¸º
> 
> $$
> \begin{bmatrix}
 0.0 & 0.0 
\end{bmatrix}
zip

\begin{bmatrix}


1 & 2


\end{bmatrix}

=
\begin{bmatrix}
 0.0 & 0.0 \\
 1 & 2
\end{bmatrix}
> $$
> 
> 
> çºµå‘ç´¯åŠ å¾—åˆ°

> $$
>  \begin{bmatrix}

 1 & 2

 \end{bmatrix}
> $$

> Â Â Â Â ä¹Ÿå°±æ˜¯`metric[0]`ä¸ºæ­£ç¡®æ•°`metric[1]`æ ·æœ¬æ€»æ•°ã€‚å‡†ç¡®ç‡ä¸º
> 
> `metric[0] / metric[1]`

è‡³æ­¤æˆ‘ä»¬å·²ç»å¯ä»¥è¯„ä¼°æ¨¡å‹å‡†ç¡®ç‡ï¼Œå¯ä»¥å¼€å§‹ç€æ‰‹è®­ç»ƒæ¨¡å‹äº†ã€‚

### è®­ç»ƒè„šæœ¬

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc 

def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]   
```

> Â Â Â Â è§£é‡Šï¼šè®­ç»ƒæ¬¡æ•°ç”±`num_epochs`æ¥æ§åˆ¶ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šè¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦å­˜æ”¾åœ¨`train_metrics`ä¸­ã€‚æµ‹è¯•ç²¾åº¦`test_acc`åˆ™æ˜¯åœ¨æ¯æ¬¡è¿­ä»£æ—¶è®¡ç®—ä¸€ä¸‹æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸­çš„å‡†ç¡®åº¦ã€‚
> 
> Â Â Â Â ç„¶åå°±æ˜¯æ¯æ¬¡çš„è¿­ä»£å…·ä½“åšäº†ä»€ä¹ˆæ“ä½œäº†ã€‚è¿™é‡ŒåŠ äº†ä¸€ä¸ªåˆ¤æ–­ï¼Œå¦‚æœæ˜¯torchè‡ªå¸¦çš„nnæ¨¡å‹å°±è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œ
> 
> Â Â Â Â ç„¶åæ¨¡å‹ç»™å‡ºè®­ç»ƒé¢„æµ‹å€¼`y_hat`ï¼Œå¹¶è®¡ç®—æŸå¤±
> 
> Â Â Â Â å€¼å¾—æ³¨æ„çš„æ˜¯torchè‡ªå¸¦nnæ¨¡å‹è¿è¡Œçš„æ˜¯ã€‚
> 
> ```python
>             updater.zero_grad()  #æ¸…é›¶æ¢¯åº¦é˜²æ­¢æ¢¯åº¦ç§¯ç´¯
>             l.mean().backward()  #å¯¹æŸå¤±å‡½æ•°å‡å€¼æ±‚æ¢¯åº¦
>             updater.step()       #è‡ªåŠ¨æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
> ```
> 
> å¦‚æœæ‰‹å†™çš„å°±è¿è¡ŒğŸ‘‡
> 
> ```python
>             l.sum().backward()     #å¯¹æŸå¤±å‡½æ•°å’Œæ±‚æ¢¯åº¦(â‘ ä¸ºä»€ä¹ˆï¼Ÿ)
>             updater(X.shape[0])    #æŒ‡å®šbatch_sizeéšæœºæ¢¯åº¦ä¸‹é™
> ```
> 
> è¿™é‡Œç”¨`X.shape[0]`è€Œä¸æ˜¯`batch_size`æ˜¯å› ä¸ºå¯ä»¥åœ¨ä¸ä¿®æ”¹ä»£ç çš„æƒ…å†µä¸‹é€‚åº”ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ•°æ®ã€‚è¿™å¯¹äºåœ¨ä¸åŒæƒ…å†µä¸‹çµæ´»åœ°è°ƒæ•´æ‰¹æ¬¡å¤§å°æˆ–è€…åœ¨ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†æ—¶éå¸¸æ–¹ä¾¿ã€‚
> 
> Â Â Â Â æœ€åç´¯åŠ å™¨è®¡ç®—å½“å‰è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦ã€‚

â‘ åœ¨å•æ¬¡è¿­ä»£è®­ç»ƒä»£ç ä¸­æˆ‘ä»¬å¾ˆå®¹æ˜“å¯ä»¥å‘ç°ä¸ºä»€ä¹ˆè‡ªåŠ¨æ±‚çš„æ˜¯`l.mean().backward()`

è€Œæ‰‹å†™çš„æ˜¯`l.sum().backward()`ï¼Œæ ¹æ®å…¬å¼ä»¥åŠç›´è§‰æ±‚æŸå¤±å‡½æ•°çš„å‡å€¼çš„æ¢¯åº¦ä¼¼ä¹æ¯”æ±‚å’Œè¦å¥½ã€‚è¿™æ˜¯å› ä¸ºè¿™æ ·è®¡ç®—çš„æ¢¯åº¦æ›´ä¸ºç¨³å®šï¼Œä¸ä¼šå—åˆ°æ‰¹æ¬¡å¤§å°çš„å½±å“ã€‚

Â Â Â Â ä½†æ˜¯æ³¨æ„å›å¿†æ¢¯åº¦ä¸‹é™ï¼Œ

![ ](images\2024-01-26-18-02-27-image.png)

Â Â Â Â æ¢¯åº¦ä¸‹é™æ—¶å€™ï¼Œè¿˜è¦å‡å»ä¸€ä¸ª`lr`å€çš„æ¢¯åº¦ï¼Œè€Œåœ¨æ±‚æŸå¤±å‡½æ•°çš„å…¬å¼ä¸€èˆ¬æ˜¯è¦æ±‚æˆ‘ä»¬æ±‚å‡å€¼çš„ã€‚å¦‚çº¿æ€§å›å½’ï¼š

$$
â„“(\hat{y}, y) = â„“(\mathbf{X},\mathbf{w},b,\mathbf{y} )\\

                                                                 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â =\frac{1}{2n}\sum_{i=1}^{n}{(<\mathit{\mathbf{x_i}},\mathbf{w}> + b - y_i)^2}\\

                       Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â =\frac{1}{2n}\|\mathbf{X}\mathbf{w}+b-\mathbf{y} \|^2
$$

Â Â Â Â æˆ‘ä»¬å»æŠŠæ‰‹å†™çš„æ¢æˆ`mean()`è¯•ä¸€è¯•ä¼šä¸ä¼šæ›´å¥½å‘¢ã€‚

![](.\images\2024-01-28-20-25-20-image.png)

ç»“æœæƒ¨è´¥ï¼Œç²¾åº¦å®Œå…¨ä¸å¤Ÿã€‚è¿™æ˜¯å› ä¸º`sum()`æ”¹æˆ`mean()`ä¹‹åå­¦ä¹ ç‡`lr`æ²¡æœ‰ä¿®æ”¹ï¼Œå› ä¸ºæŸå¤±å‡½æ•°å€¼çš„ç³»æ•°æ˜¯ç”±`lr`å’Œ`n`å…±åŒå†³å®šçš„ï¼Œ`sum()`æ”¹æˆ`mean()`~~ï¼ˆä¹Ÿå°±æ˜¯å‡å€¼ï¼‰~~ä¹‹åï¼Œç›¸å½“äºå¢å¤§äº†è®©æœ¬æ¥åˆšåˆšå¥½åˆé€‚çš„æ¢¯åº¦å˜å°äº†ä½†æ˜¯å­¦ä¹ ç‡æ²¡å˜å¯¼è‡´å•æ­¥ä¸‹é™çš„æ­¥é•¿è¿‡å°äº†ï¼Œ10æ¬¡epochå®Œå…¨ä¸èƒ½è¾¾åˆ°ç²¾åº¦è¦æ±‚ã€‚

Â Â Â Â åšä¸ªå®éªŒï¼šå¦‚æœä¸€æ‰¹æ˜¯256ä¸ªæ ·æœ¬ï¼Œ`(batch_size=256)`ï¼Œé‚£ä¹ˆæ”¹ä¸º`mean()`çš„è¯ï¼Œä¸ºäº†ä¿æŒåŸæ¥çš„ç²¾åº¦æˆ‘çš„`lr`åº”å½“å¢å¤§255å€ã€‚

![](images\2024-01-28-20-36-25-image.png)

æ”¹ä¸ºğŸ‘‡

![](images\2024-01-28-20-36-55-image.png)

ç»“æœğŸ‘‡`test_acc`ç²¾ç¡®åº¦å·²ç»æ¥åˆ°äº†83%æ¥è¿‘åŸæ¥çš„æ°´å¹³ã€‚

![](images\2024-01-28-20-38-05-image.png)

Â Â Â Â ä¸ºä»€ä¹ˆç”¨`torch`è‡ªå¸¦çš„æ¨¡å‹å°±å¯ä»¥ç›´æ¥ç”¨`mean()`(å‡å€¼)ï¼Œå› ä¸ºå®ƒçš„`updater.step()`æ˜¯è‡ªåŠ¨çš„ã€‚ã€‚ã€‚

### é¢„æµ‹æ ‡ç­¾ä»¥åŠæ˜¾ç¤ºé¢„æµ‹ç»“æœ

Â Â Â Â è®­ç»ƒå®Œäº†æ¨¡å‹ï¼Œå°±å¯ä»¥è®©å®ƒé¢„æµ‹ä¸€ä¸‹çœ‹çœ‹äº†ï¼Œä¸ºäº†å¯è§†åŒ–å†™äº†ä¸€ä¸ª`show_images`ï¼ˆå‡ºè‡ªchatGptï¼‰ï¼Œé¢„æµ‹ç»“æœå…¶å®åªéœ€è¦æŠŠè®­ç»ƒç»“æŸçš„`y_hat`çš„æ¨ªå‘æœ€å¤§å€¼ï¼Œä¹Ÿå°±æ˜¯å•ä¸ªæ ·æœ¬æ¦‚ç‡æœ€å¤§çš„å–å‡ºå³å¯ã€‚

```python
def show_images(images, num_rows, num_cols, titles=None, scale=1.5):
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, images)):
        ax.imshow(img,)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    plt.show()

def predict_ch3(net, test_iter, n=6):  #@save
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
```

## è¿è¡Œ

æœ€ååœ¨ä¸»å‡½æ•°ï¼Œè®©ä»–é©±åŠ¨èµ·æ¥

```python
if __name__ == '__main__':
    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
    predict_ch3(net, test_iter)
```

ç»“æœğŸ‘‡ï¼Œå‰å…­ä¸ªå…¨éƒ¨é¢„æµ‹æ­£ç¡®ï¼Œè¿æ°”ä¸é”™ã€‚

![](images\2024-01-28-20-46-49-image.png)

> æœ‰ä¸€ä¸ªpythonè¯­æ³•å¯èƒ½ä¼šå¸¦æ¥ç–‘æƒ‘

> ```python
>   train_ch3(net, train_iter, test_iter,cross_entropy, 
>   Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â num_epochs, updater)
> ```
> 
> æ•´ä¸ªä»£ç éƒ½æ²¡æœ‰å®šä¹‰`net`ã€`cross_entropy`ã€`updater`ï¼Œè¿™é‡Œå‡½æ•°è°ƒç”¨ç›´æ¥ç”¨ä»–ä»¬å´æ²¡æŠ¥é”™ã€‚è¿™é‡Œå®ƒä»¬æ˜¯ä½œä¸ºå‚æ•°è¢«ä¼ ç»™äº†å‡½æ•°ï¼Œç”±äºä»–ä»¬åˆæ˜¯å‡½æ•°åï¼Œæ‰€ä»¥`python`åˆ¤å®šå®ƒä»¬å¯ä»¥æœ€ç»ˆæ‰¾åˆ°è°ƒç”¨å‡½æ•°çš„åœ°æ–¹ï¼Œåœ¨æ­¤ä¹‹å‰ä¸€ç›´ä½œä¸ºå‚æ•°ä¼ é€’ï¼Œå‘å‰ä¼ æ’­ï¼Œç›´åˆ°è°ƒç”¨äº†åŒåå‡½æ•°ã€‚
> 
> è¿™ç§æ–¹å¼çš„å¥½å¤„æ˜¯ä½ å¯ä»¥åœ¨ä¸åŒçš„åœ°æ–¹å®šä¹‰ä¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä¸”åªéœ€è¦ç¡®ä¿è¿™äº›æ¨¡å‹å…·æœ‰ç›¸åŒçš„æ¥å£ï¼ˆå³å¯ä»¥æ¥å—è¾“å…¥å¹¶è¿”å›è¾“å‡ºï¼‰ã€‚è¿™ä½¿å¾—ä»£ç æ›´åŠ çµæ´»ï¼Œå¯ä»¥è½»æ¾åœ°åˆ‡æ¢å’Œæµ‹è¯•ä¸åŒçš„æ¨¡å‹ã€‚
> 
> ä¸¾ä¸ªç®€å•ä¾‹å­ï¼š
> 
> ```python
> def dog(a):
>     return a
> def dogMaster(dog, saying):
>     print(saying, dog("popy"))
> 
> dogMaster(dog,"come on")
> ```
> 
> Â Â Â Â æœ‰ä¸€ä¸ªå‡½æ•°`dog(a)`ä½œç”¨æ˜¯è¿”å›å‚æ•°å€¼ã€‚å‡½æ•°`dogMaster(dog, saying)`ä½œç”¨æ˜¯è¾“å‡º`saying`,`dog("popy")`ï¼Œæ²¡æœ‰æ˜¾ç¤ºå®šä¹‰`dog`æ˜¯ä»€ä¹ˆï¼Œä½†æ˜¯`python`å‘ç°æœ‰ä¸ªå‡½æ•°å«`dog`ï¼Œå®ƒå°±ä¼šæŠŠ`dog`ä½œä¸ºå‚æ•°ä¼ é€’ç»™`dogMaster`ï¼Œè€Œåœ¨`dogMaster`ä¸­æ‰¾åˆ°äº†è°ƒç”¨`dog("popy")`è¿”å›`â€œpopyâ€`ï¼Œæ‰€ä»¥ç»“æœåº”è¯¥ä¸º`come on popy`
> 
> ![](.\images\2024-01-28-21-00-47-image.png)

## å®Œæ•´ä»£ç 

```python
import torch
from d2l import torch as d2l
import matplotlib.pyplot as plt


batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

num_inputs = 784
num_outputs = 10

W = torch.normal(0,0.01,size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)

X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])

def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1,keepdim=True)
    return X_exp / partition

def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)


def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])

def accuracy(y_hat, y):
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡ã€‚"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())


def evaluate_accuracy(net, data_iter):
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦ã€‚"""
    if isinstance(net, torch.nn.Module):
        net.eval()
    metric = Accumulator(2)
    for X, y in data_iter:
        metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]

class Accumulator:
    """åœ¨`n`ä¸ªå˜é‡ä¸Šç´¯åŠ ã€‚"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.mean().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]


lr = 25.6

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        print(test_acc)
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc

def show_images(images, num_rows, num_cols, titles=None, scale=1.5):
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, images)):
        ax.imshow(img,)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    plt.show()

def predict_ch3(net, test_iter, n=6):  #@save
    """é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

if __name__ == '__main__':
    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
    predict_ch3(net, test_iter)
```
