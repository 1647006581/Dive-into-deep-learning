# <u>Dive-Into-Deeplearningå­¦ä¹ ç¬”è®° </u>

# çº¿æ€§å›å½’

Â Â Â Â Â Â Â Â çº¿æ€§å›å½’æ˜¯ä¸€ç§ç®€å•çš„å›å½’æ¨¡å‹ï¼Œå…¶ä½œç”¨æ˜¯é¢„æµ‹ä¸€ä¸ªçº¿æ€§çš„è¿ç»­

<img src="file:///C:/Users/ASUS/AppData/Roaming/marktext/images/2024-01-26-15-40-08-image.png" title="" alt="" width="590">

## çº¿æ€§æ¨¡å‹

Â Â Â Â Â Â Â Â å‡è®¾nç»´è¾“å…¥å‘é‡
        $$
        \mathbf{x} = [x_1, x_2, \ldots, x_n]^T 
        $$

Â Â Â Â Â Â Â Â è¾“å‡ºç»“æœä¸º $$\mathbf{y}=w_1 * x_1 + w_2 *x_2 + ...+w_n*x_n + b$$    

Â Â Â Â Â Â Â Â $w$è¡¨ç¤º weight æƒé‡   $b$è¡¨ç¤ºbias åç§»

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â å…¶ä¸­Â $$\mathbf{w} = [w_1, w_2, \ldots, w_n]^T$$

Â Â Â Â Â Â Â Â Â è¿™é‡Œ çº¿æ€§å›å½’è¦åšçš„ä»»åŠ¡å°±æ˜¯é€šè¿‡å·²æœ‰çš„ä¸€äº›ç¦»æ•£çš„ç‚¹å»æ‹Ÿåˆå‡ºä¸€ç»„$\mathbf{w},b$ å¾—åˆ°ä¸€æ¡æ›²çº¿ç”¨ä»¥é¢„æµ‹å…³äºå¯¹åº”ç‰¹å¾çš„å…³ç³»

Â Â Â Â Â Â Â Â $\hat{y}=<\mathbf{w},\mathbf{x}>+  b$

## è®­ç»ƒæ•°æ®

Â Â Â Â Â Â Â Â å‡è®¾æœ‰nä¸ªæ ·æœ¬è®°ä¸º:(<mark>æ³¨æ„è¿™é‡Œ$X$ å’Œå‰é¢æ‰€è¿°çš„å‘é‡ $x$ä¸ä¸€æ · </mark>ï¼Œ<mark>æ­¤å¤„ä¸ºæ ·æœ¬ å¯ä»¥ç†è§£ä¸ºä¸‹æ–¹çš„$x_i$ä¸ºä¸€ä¸ªå‰é¢è¯´çš„å‘é‡$x$)</mark>
Â Â Â Â Â Â Â Â $X = [x_1, x_2, â€¦, x_n]^T  ,y = [y_1, y_2, â€¦, y_n]^T$

## æŸå¤±å‡½æ•°

Â Â Â Â Â Â Â è¯„ä¼°æ¨¡å‹è®­ç»ƒå¥½åæœ€ç›´è§‚çš„æ–¹å¼å°±æ˜¯æŸ¥çœ‹é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®å€¼

Â Â Â Â Â Â Â Â Â Â Â Â å¹³æ–¹æŸå¤±å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š~~(äºŒåˆ†ä¹‹ä¸€æ˜¯å› ä¸ºå¯ä»¥æ±‚å¯¼æ¶ˆå»ï¼Œå…¶å®æ— æ‰€è°“æ˜¯å¤šå°‘)~~

Â Â Â Â Â Â Â Â Â Â Â Â $â„“(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2$

## ä¼˜åŒ–ç®—æ³•

Â Â Â Â Â Â Â Â æœ‰äº†æ¨¡å‹å’ŒæŸå¤±å‡½æ•°ï¼Œç›¸å½“äºæœ‰äº†ææ–™å’Œå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸå¤±å‡½æ•°è¯„ä¼°ç›®å‰æ¨¡å‹è®­ç»ƒå¥½åï¼Œä½†æ˜¯è¿˜éœ€è¦ä¸€ä¸ªè¾¾åˆ°æˆ‘ä»¬å¯æ¥å—çš„æŸå¤±çš„æ–¹æ³•  ~~ï¼ˆä¸»è§‚èƒ½åŠ¨æ€§ï¼‰~~ï¼Œ

Â Â Â Â Â Â Â Â å¯¹äºçº¿æ€§å›å½’ï¼Œæˆ‘ä»¬å›åˆ°æŸå¤±å‡½æ•°Â Â Â Â $â„“(\hat{y}, y) = \frac{1}{2} (\hat{y} - y)^2$

Â Â Â Â Â Â Â Â ä¹Ÿå°±æ˜¯$\frac{1}{2} (\hat{y} - y)^2 = \frac{1}{2} (w_1 * x_1 + w_2 *x_2 + ...+w_n*x_n + b - y)^2$

Â Â Â Â Â Â Â Â ä¸ºäº†æ–¹ä¾¿æ¨å¯¼,ç®€å†™ä¸ºå†…ç§¯æ ¼å¼$\frac{1}{2} (\hat{y} - y)^2 = \frac{1}{2} (<\mathit{\mathbf{x}},\mathbf{w}> + b - y)^2$

Â Â Â Â Â Â Â Â Â 

Â Â Â Â Â Â Â Â æœ‰äº†è®­ç»ƒæ•°æ® ä»¥åŠç­‰å¾…é¢„æµ‹å‚æ•°$\mathbf{w}$å’Œ$b$ï¼ŒæŸå¤±å‡½æ•°å¯ä»¥å…·ä½“å†™ä¸ºï¼š

Â Â Â Â Â Â Â Â $â„“(\hat{y}, y) = â„“(\mathbf{X},\mathbf{w},b,\mathbf{y} )$

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â $=\frac{1}{2n}\sum_{i=1}^{n}{(<\mathit{\mathbf{x_i}},\mathbf{w}> + b - y_i)^2}$ 

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â $=\frac{1}{2n}\|\mathbf{X}\mathbf{w}+b-\mathbf{y}  \|^2$

>  Â Â Â Â Â Â Â Â ä¸ºä»€ä¹ˆè¦â—n ï¼Ÿ ä¸ºäº†è·å¾—å¹³å‡æŸå¤±ï¼Œåœ¨è¯„ä¼°ç²¾åº¦çš„æ—¶å€™å¥½ä¸€äº›ã€‚

Â Â Â Â Â Â Â Â æ­¤æ—¶æˆ‘ä»¬å¾—åˆ°æœ€ç»ˆçš„æŸå¤±å‡½æ•°äº†ï¼Œå› ä¸ºæˆ‘ä»¬è¦è®­ç»ƒå‡º$\mathbf{w}$å’Œ$b$çš„å€¼ï¼Œä¼˜åŒ–æ˜¯é’ˆå¯¹è¿™ä¸¤ä¸ªå‚æ•°çš„ï¼Œä¸è¦é™·å…¥æ€ç»´å®šåŠ¿ç›¯ç€*X* å’Œ *y*ã€‚

> Â Â Â Â Â Â Â Â è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼Œè®­ç»ƒå¹¶æ‰¾å‡ºå‡ºä¸€å¯¹ç²¾åº¦å¯ä»¥æ¥å—çš„$\mathbf{w}$å’Œ$b$ï¼Œç”¨äºæ‹Ÿåˆ**y**ä¸**X**çš„çº¿æ€§å…³ç³»Â 

### GDæ¢¯åº¦ä¸‹é™ç®—æ³•

Â Â Â Â Â Â Â Â è§‚å¯ŸæŸå¤±å‡½æ•°ï¼Œ$\ell$  å…³äº$w$çš„åå¯¼ï¼Œ$\ell$ å…³äº$b$çš„åå¯¼  ~~(å¤ªéº»çƒ¦ä¸åŠ ç²—äº†ï¼Œè€Œä¸”ä¸åº”è¯¥å«åå¯¼åº”è¯¥å«æ¢¯åº¦)~~

Â Â Â Â Â Â Â Â æŠŠè¿™ä¸¤ä¸ªå‘é‡çœ‹æˆä¸¤ä¸ªå˜é‡è¯ï¼Œå¯ä»¥å¾—åˆ°$\ell$æœ‰åªä¸€ä¸ªæå€¼ç‚¹ï¼Œä¸ç”¨åˆ¤æ–­äº†åªèƒ½æ˜¯è¿™ä¸ªç‚¹äº†ï¼Œæ‰€ä»¥ç°åœ¨çš„ç›®æ ‡å°±æ˜¯ä¸æ–­é€¼è¿‘è¿™ä¸ªâ€œç‚¹â€

> Â Â Â Â Â Â Â Â è¿™é‡Œæ˜¯æ¨å¯¼è¿‡ç¨‹ï¼š<img src="file:///C:/Users/ASUS/AppData/Roaming/marktext/images/2024-01-26-18-00-42-image.png" title="" alt="" width="594">

Â Â Â Â Â Â Â Â æ¥ä¸‹æ¥å°±å¼•å‡ºäº†æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨$w$ä¸¾ä¾‹ï¼Œé€‰å®šä¸€ä¸ªåˆå§‹å€¼$w_0$ æ­¤æ—¶ä¸€å®šæ˜¯å’Œè¿™ä¸ªæœ€ä¼˜æˆ–è€…è¯´è¾ƒä¼˜è§£æ˜¯æ¯”è¾ƒè¿œçš„ï¼Œè€Œå¿«é€Ÿå‘è§£é€¼è¿‘çš„æ–¹å‘å°±æ˜¯æ¢¯åº¦æ–¹å‘ï¼ˆæ¢¯åº¦æ€§è´¨ï¼‰ï¼Œæ­¤æ—¶æˆ‘ä»¬åªéœ€è¦ä¸æ–­ç”¨ä¸Šä¸€ä¸ªå‘é‡$w$å‡å»å½“å‰å‘é‡$w$

> æ¢¯åº¦çš„æ–¹å‘æŒ‡å‘å‡½æ•°åœ¨ç»™å®šç‚¹ä¸Šå¢åŠ æœ€å¿«çš„æ–¹å‘ã€‚æ¢¯åº¦çš„åæ–¹å‘æŒ‡å‘å‡½æ•°å‡å°æœ€å¿«çš„æ–¹å‘ã€‚

> <img title="" src="file:///C:/Users/ASUS/AppData/Roaming/marktext/images/2024-01-26-18-02-27-image.png" alt="" width="491">
> 
> å°†è¿‡ç¨‹ç”»å‡ºæ¥ï¼š
> 
> <img src="file:///C:/Users/ASUS/AppData/Roaming/marktext/images/2024-01-26-18-24-51-image.png" title="" alt="" width="378">

Â Â Â Â Â Â Â Â å½“ç„¶ï¼Œä¸ºäº†è¿­ä»£å¯æ§ï¼Œå¯ä»¥å¼•å…¥ä¸€ä¸ªå‚æ•°ï¼Œå­¦ä¹ ç‡æ¥æ§åˆ¶æ­¥é•¿ã€‚å­¦ä¹ ç‡è¿‡å¤§è¿‡å°éƒ½å¯èƒ½å¼•å‘é—®é¢˜ï¼Œå­¦ä¹ ç‡è¿‡å°å¯èƒ½ä¼šå¯¼è‡´è®¡ç®—é‡è¿‡å¤§ä¼šå ç”¨å¤§é‡çš„èµ„æºï¼Œå­¦ä¹ ç‡è¿‡å¤§å¯èƒ½ä¼šå› ä¸ºæ­¥é•¿è¿‡å¤§æ— æ³•è·å¾—è¾ƒç²¾ç¡®è§£ï¼Œæˆ–è€…æ˜¯å¯¼è‡´æ±‚å¯¼è¿‡ç¨‹ä¸­å‡ºç°â—0ã€‚

## å…³é”®ä»£ç å®ç°

pytorchå®ç°ï¼Œç”¨åˆ°çš„å‡½æ•°å¦‚ä¸‹

```python
def linreg(X,w,b):
"""å»ºç«‹æ¨¡å‹"""
    return torch.matmul(X, w) + b
```

```python
def square_loss(y_hat,y):
"""æŸå¤±å‡½æ•°"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

#### æ•°æ®è¿­ä»£å™¨

Â Â Â Â å®šä¹‰ä¸€ä¸ªæ•°æ®è¿­ä»£å™¨å®ç°éšæœºåˆ†æ‰¹æ¬¡ï¼Œ*yield*å…³é”®å­—çš„ä½œç”¨ï¼šç”¨äºå®šä¹‰ç”Ÿæˆå™¨å‡½æ•°ã€‚ç”Ÿæˆå™¨å‡½æ•°ä¸æ™®é€šå‡½æ•°ä¸åŒï¼Œå®ƒçš„æ‰§è¡Œæ˜¯å»¶è¿Ÿçš„ï¼Œåªæœ‰åœ¨éœ€è¦æ—¶æ‰ä¼šäº§ç”Ÿä¸€ä¸ªå€¼ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¯¹æ¯”

```python
def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices [i : min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]
```

Â Â Â Â ä¸¤è€…çš„ä¸åŒå°†åœ¨è°ƒç”¨æ—¶ä½“ç°

```python
def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices [i : min(i+batch_size,num_examples)])
        return features[batch_indices],labels[batch_indices]
#è°ƒç”¨å¿…é¡»æœ‰ç”³è¯·ç©ºé—´å…¨éƒ¨å­˜ä¸‹
a = data_iter(batch_size,features,labels)
```

#### å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

```python
def sgd(params,lr,batch_size):
    """å°æ‰¹é‡æ¢¯åº¦ä¸‹é™"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

#### è®­ç»ƒ

```python
"""åˆå§‹åŒ–"""
batch_size = 10
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

num_epoch = 5
lr = 0.03
net = linreg
loss = square_loss
for epoch in range(num_epoch):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

Â Â Â Â `for X, y in data_iter(batch_size, features, labels):`è¿™é‡Œä½“ç°å‡ºäº†ç”Ÿæˆå™¨å‡½æ•°çš„ä½œç”¨ï¼Œå¦‚æœä½ ä¸ä½¿ç”¨ `yield`ï¼Œè€Œæ˜¯åœ¨å‡½æ•°å†…éƒ¨ç›´æ¥è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¹æ¬¡æ•°æ®çš„æ•°æ®ç»“æ„ï¼Œæ¯”å¦‚åˆ—è¡¨ï¼Œé‚£ä¹ˆä½ éœ€è¦ä¸€æ¬¡æ€§å°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜ä¸­ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´å†…å­˜ä¸è¶³çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ã€‚

Â Â Â Â `with torch.no_grad()`ç”¨äºæŒ‡å®šä¸€æ®µä»£ç å—ï¼Œåœ¨è¿™ä¸ªä»£ç å—ä¸­ï¼ŒPyTorchä¼šå…³é—­æ¢¯åº¦è®¡ç®—ã€‚å› ä¸ºwï¼Œbåœ¨åˆ›å»ºæ—¶`requires_grad = True`åœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼ŒPyTorchä¼šè‡ªåŠ¨è®¡ç®—è¿™äº›æ¢¯åº¦ï¼Œåœ¨è¿›è¡Œæ¨¡å‹å‚æ•°æ›´æ–°çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸å†éœ€è¦ç»§ç»­ä¿ç•™ä¹‹å‰è®¡ç®—çš„æ¢¯åº¦ï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒä½¿ç”¨å½“å‰æ¢¯åº¦è¿›è¡Œå‚æ•°æ›´æ–°ã€‚ä¸ºäº†å‡å°‘å†…å­˜æ¶ˆè€—å’Œè®¡ç®—å¼€é”€ï¼Œé€šå¸¸ä¼šåœ¨è¿›è¡Œå‚æ•°æ›´æ–°æ—¶æ¸…é›¶ä¹‹å‰è®¡ç®—çš„æ¢¯åº¦ï¼Œä»¥é˜²æ­¢æ¢¯åº¦ä¿¡æ¯ç´¯ç§¯ã€‚ä¹Ÿå°±æ˜¯sgdå‡½æ•°ä¸­çš„`param.grad.zero_()`

## å®Œæ•´ä»£ç ä»¥åŠç»“æœ

```python
from matplotlib import pyplot as plt
import random
import torch as tc
import numpy as np
# y = wx + b
num_inputs = 2
num_examples = 1000
true_w = [2,-3.4]
true_b = 4.2
features = tc.randn((num_examples,num_inputs))
labels = true_w[0] * features[:,0] + true_w[1] * features[:,1] + true_b
labels += tc.normal(mean=0, std=0.01,size=labels.shape)

plt.scatter(features[:,0].numpy(),labels.numpy(),s=1)
plt.xlabel("Feature 1")
plt.ylabel("Labels")
plt.title("Scatter Plot")
plt.scatter(features[:,1].numpy(),labels.numpy(),edgecolors='r',s=1)
plt.legend

def data_iter(batch_size,features,labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = tc.tensor(indices [i : min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]




def square_loss(y_hat,y):
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

def linreg(X,w,b):
    return tc.matmul(X, w) + b

def sgd(params,lr,batch_size):
    """å°æ‰¹é‡æ¢¯åº¦ä¸‹é™"""
    with tc.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()



batch_size = 10
w = tc.normal(0, 0.01, size=(2,1), requires_grad=True)
b = tc.zeros(1, requires_grad=True)

num_epoch = 5
lr = 0.03
net = linreg
loss = square_loss
ind = 0
for epoch in range(num_epoch):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with tc.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')



x_val = np.arange(features[:, 1].min(), features[:, 1].max(),0.1)

w_val = w[1].detach().numpy().item()
b_val = b.detach().numpy()
y_val = w_val * x_val + b_val
print(y_val)

plt.plot(x_val, y_val)
plt.legend
plt.show()
```

ä»£ç ä¸­éœ€è¦æ³¨æ„çš„æ˜¯torchçš„ç”Ÿæˆè‡ªå®šä¹‰æ ‡å‡†åˆ†å¸ƒéšæœºæ•°

`torch.randn` å‡½æ•°ç”¨äºç”Ÿæˆæœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œè€Œä¸å…è®¸ç›´æ¥è®¾ç½®æ ‡å‡†å·®ã€‚å¦‚æœä½ æƒ³ç”Ÿæˆæœä»å…¶ä»–æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œå¯ä»¥ä½¿ç”¨ `torch.normal` å‡½æ•°ã€‚

`torch.normal(mean=0, std=0.01,size=labels.shape)` å‡å€¼0 ï¼Œæ ‡å‡†å·®0.01ï¼Œå½¢çŠ¶ä¸ºlabelsçš„å½¢çŠ¶(1ï¼Œ1000)

`torch.randn(size=(2,1))`ç”Ÿæˆæœä»æ­£æ€åˆ†å¸ƒçš„å½¢çŠ¶ä¸º(2ï¼Œ1)çš„éšæœºæ•° 

ç”¨`normal`å¿…é¡»æŒ‡å®š`mean std size`çš„å‚æ•°ï¼Œç”¨`randn`ä¸èƒ½ä¿®æ”¹`mean std`

ç»“æœå¦‚ä¸‹ï¼Œå…¶é¢„æµ‹çš„æ˜¯$w_2$çš„æƒé‡ä»¥åŠåç§»$b$

<img src="file:///C:/Users/ASUS/AppData/Roaming/marktext/images/2024-01-26-20-06-57-image.png" title="" alt="" width="635">

# Softmaxå›å½’

softmaxå›å½’è™½ç„¶æ˜¯å›å½’ï¼Œå®é™…ä¸Šæ˜¯ä¸ªåˆ†ç±»é—®é¢˜

Â Â Â Â Â ç®€å•ç†è§£ï¼šç»™æ¯ä¸ªå›å½’å¾—å‡ºçš„outputæ‰“åˆ†æ¯”å¦‚[10ï¼Œ1ï¼Œ1] ç„¶åå°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œ~~ï¼ˆè¿™é‡Œæ²¡æœ‰ç”¨softmaxçš„è½¬åŒ–æ–¹å¼ï¼‰~~ è½¬æ¢[0.833,0.083,0.083] æ¦‚ç‡ç›¸åŠ ç»“æœä¸º1ï¼Œä¸”ç¬¬ä¸€ä¸ªæ¦‚ç‡è¿œå¤§äºåä¸¤ä¸ªï¼Œåˆ™åˆ†ç±»ä¸ºç¬¬ä¸€ç±»ã€‚

### åˆ†ç±»é—®é¢˜

Â Â Â Â Â Â Â 1.ç›´è§‚åšæ³•ï¼š

æœ€ç›´æ¥çš„æƒ³æ³•æ˜¯é€‰æ‹©$y=\{1,2,3\}$ï¼Œå…¶ä¸­æ•´æ•°åˆ†åˆ«ä»£è¡¨ç‹—çŒ«é¸¡ã€‚
è¿™æ˜¯åœ¨è®¡ç®—æœºä¸Šå­˜å‚¨æ­¤ç±»ä¿¡æ¯çš„æœ‰æ•ˆæ–¹æ³•ã€‚ å¦‚æœç±»åˆ«é—´æœ‰ä¸€äº›è‡ªç„¶é¡ºåºï¼Œ
æ¯”å¦‚è¯´æˆ‘ä»¬è¯•å›¾é¢„æµ‹$\{å©´å„¿,å„¿ç«¥,é’å°‘å¹´,é’å¹´äºº,ä¸­å¹´äºº,è€å¹´äºº\}$ï¼Œ
é‚£ä¹ˆå°†è¿™ä¸ªé—®é¢˜è½¬å˜ä¸ºå›å½’é—®é¢˜ï¼Œå¹¶ä¸”ä¿ç•™è¿™ç§æ ¼å¼æ˜¯æœ‰æ„ä¹‰çš„ã€‚

<mark>ä½†æ˜¯å¤§å¤šæ•°åˆ†ç±»é—®é¢˜éƒ½ä¸ä¼šæ˜¯è¿™æ ·</mark>

Â Â Â Â Â Â Â Â 2.Â *ç‹¬çƒ­ç¼–ç *ï¼ˆone-hot encodingï¼‰ï¼š

æ¯”å¦‚ä¸€ä¸ªå‘é‡$[x_1,x_2,x_3]$åˆ†åˆ«ä»£è¡¨(çŒ« ï¼Œç‹— ï¼Œ é¸¡)   $[1,0,0]$ä»£è¡¨ğŸ±ï¼Œ$[0,1,0]$ä»£è¡¨ğŸ¶ï¼Œ$[0,0,1]$ä»£è¡¨ğŸ¥

Â Â Â Â Â Â Â Â æ‰€ä»¥è¾“å‡ºä¸º$y=\{(1,0,0),(0,1,0),(0,0,1)\}$

### ç¥ç»ç½‘ç»œ

Â Â Â Â Â Â Â Â è¿˜æ˜¯ç”¨çŒ«ç‹—é¸¡åˆ†ç±»æ¥è¯´ï¼Œå‡è®¾çŒ«ç‹—é¸¡åˆ†ç±»æœ‰å‘é‡$\mathbf{x}=[x_1,x_2,x_3,x_4]$ä½œä¸º`ç‰¹å¾å‘é‡`

<mark>ä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬$X$æœ‰4ä¸ªç‰¹å¾</mark>

Â Â Â Â Â Â Â  æœ‰æƒé‡çŸ©é˜µ$W$,åç½®å‘é‡$\mathbf{b}$

$$
W=\begin{bmatrix}
  \beta_1\\ 
 \beta_2\\
 \beta_3\\
\end{bmatrix}

=
\begin{bmatrix}
  w_{11} & w_{12} & w_{13} & w_{14}\\
  w_{21} & w_{22} & w_{23} & w_{24}\\
  w_{31} & w_{32} & w_{33} & w_{34}\\
\end{bmatrix},
b=\begin{bmatrix}
  b_1\\ 
 b_2\\
 b_3\\
\end{bmatrix}
$$

Â Â Â Â Â Â Â Â Â å¯¹äºè¾“å‡º$y$æœ‰$o_1 ä»£è¡¨ğŸ±,o_2ä»£è¡¨ğŸ¶,o_3ä»£è¡¨ğŸ¥$

Â Â Â Â Â Â Â  Â å¯¹äºæƒé‡å’Œåç½®$\beta_1æ˜¯ğŸ±çš„æƒé‡ï¼Œb_1æ˜¯ğŸ±çš„åç½®$

$$
o_1=\mathbf{x}\beta_1^T+b_1 = x_1*w_{11}+x_2*w_{12}+x_3*w_{13}+x_4*w_{14} +b_1\\
o_2=\mathbf{x}\beta_2^T+b_2 = x_1*w_{21}+x_2*w_{22}+x_3*w_{23}+x_4*w_{24}   +b_2\\
o_3=\mathbf{x}\beta_3^T+b_3= x_1*w_{31}+x_2*w_{32}+x_3*w_{33}+x_4*w_{34}+b_3
$$

Â Â Â Â Â Â Â Â Â è¿™å…¶å®æ˜¯ä¸‰æ¬¡çº¿æ€§å›å½’é—®é¢˜ã€‚çº¿æ€§å›å½’æ˜¯å•å±‚ç¥ç»ç½‘ç»œï¼Œè€Œsoftmaxç¥ç»ç½‘ç»œä¹Ÿæ˜¯å•å±‚çš„ç¥ç»ç½‘ç»œã€‚

> Â Â Â Â Â Â Â Â <img src="file:///C:/Users/ASUS/AppData/Roaming/marktext/images/2024-01-27-15-25-38-image.png" title="" alt="" width="528">

Â Â Â Â Â Â Â Â é‚£ä¹ˆæˆ‘ä»¬å¯¹å½“å‰è¿™ä¸ªæ ·æœ¬åˆ†åˆ«è®¡ç®—å‡º$o_1,o_2,o_3$çš„å€¼ï¼Œç±»ä¼¼äºæ‰“åˆ†ï¼Œå¯ä»¥ä¸ºå½“å‰æ ·æœ¬å¾—å‡ºåœ¨åƒğŸ±ï¼ŒğŸ¶ï¼ŒğŸ¥æ–¹é¢çš„ä¸‰ä¸ªä¸åŒçš„åˆ†æ•°ã€‚

### Softmax

Â Â Â Â Â Â Â Â è·å¾—äº†ğŸ±ğŸ¶ğŸ¥çš„åˆ†æ•°ç†è®ºä¸Šå·²ç»å¯ä»¥åˆ†ç±»å‡ºæ¥äº†ï¼Œåªéœ€è¦æ¯”å¦‚ğŸ±çš„åˆ†æ•°è¿œå¤§äºğŸ¶ğŸ¥çš„åˆ†æ•°ï¼Œå°±å¯ä»¥å°†å…¶åˆ†ç±»ä¸ºğŸ±ã€‚

Â Â Â Â Â Â Â Â ä½†æ˜¯ä»¥ä¸‹æ˜¯ç›´æ¥åˆ†ç±»çš„ç¼ºç‚¹ï¼š(å‡ºè‡ªchatGpt)

> 1. **èŒƒå›´ä¸ä¸€è‡´ï¼š** è¾“å‡ºå€¼çš„èŒƒå›´å¯èƒ½ä¸ä¸€è‡´ï¼Œä¸åŒç±»åˆ«çš„è¾“å‡ºå¯èƒ½å¤„äºä¸åŒçš„æ•°å€¼å°ºåº¦ä¸Šï¼Œä½¿å¾—éš¾ä»¥æ¯”è¾ƒå®ƒä»¬çš„é‡è¦æ€§æˆ–ç½®ä¿¡åº¦ã€‚
> 
> 2. **æ— æ³•è§£é‡Šä¸ºæ¦‚ç‡ï¼š** ç›´æ¥çš„è¾“å‡ºå€¼æœªç»è¿‡å½’ä¸€åŒ–ï¼Œå› æ­¤ä¸èƒ½è¢«è§£é‡Šä¸ºæ¦‚ç‡ã€‚æ¦‚ç‡åº”è¯¥åœ¨0åˆ°1ä¹‹é—´ï¼Œå¹¶ä¸”æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ä¹‹å’Œåº”ä¸º1ã€‚ç›´æ¥æ¯”è¾ƒè¾“å‡ºå€¼æ— æ³•æä¾›è¿™ç§æ¦‚ç‡è§£é‡Šã€‚
> 
> 3. **æ•°å€¼ä¸ç¨³å®šæ€§ï¼š** ç›´æ¥æ¯”è¾ƒåŸå§‹è¾“å‡ºå€¼å¯èƒ½å—åˆ°æ•°å€¼ä¸ç¨³å®šæ€§çš„å½±å“ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¯èƒ½ä¼šå‡ºç°æ•°å€¼æº¢å‡ºæˆ–ä¸‹æº¢çš„é—®é¢˜ã€‚Softmaxé€šè¿‡æŒ‡æ•°è¿ç®—å’Œå½’ä¸€åŒ–è¿‡ç¨‹ï¼Œæœ‰åŠ©äºå¤„ç†è¿™äº›æ•°å€¼ç¨³å®šæ€§çš„é—®é¢˜ã€‚
> 
> 4. **æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼š** åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç›´æ¥æ¯”è¾ƒåŸå§‹è¾“å‡ºå€¼å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œè¿™åœ¨åå‘ä¼ æ’­ä¸­å¯èƒ½å½±å“æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§ã€‚
> 
> 5. **ä¸é€‚ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼š** ç›´æ¥æ¯”è¾ƒè¾“å‡ºå€¼å¯¹äºå¤šåˆ†ç±»é—®é¢˜è€Œè¨€ï¼Œå¯èƒ½æ— æ³•æä¾›ä¸€ä¸ªæ¸…æ™°çš„å†³ç­–æ ‡å‡†ã€‚Softmaxé€šè¿‡å°†è¾“å‡ºæ˜ å°„åˆ°ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæœ‰åŠ©äºåœ¨å¤šåˆ†ç±»é—®é¢˜ä¸­åšå‡ºå†³ç­–ã€‚

Â Â Â Â Â Â Â Â ç®€è€Œè¨€ä¹‹ï¼Œå°†åŸå§‹è¾“å‡ºå€¼è½¬åŒ–ä¸ºæ¦‚ç‡æ›´åŠ æœ‰åˆ©ã€‚

#### Softmaxè¿ç®—

<div>
<span style="font-size:larger;">
</span>

</div>

$$
\hat{y_i}=Softmax(o_i)=\frac{e^{o_i}}{\sum_{k=1}^n{e^{o_k}}}
$$



Â Â Â Â Â Â Â Â ç°åœ¨æˆ‘ä»¬æŠŠ$\mathbf{o}$è½¬åŒ–ä¸ºäº†$\mathbf{\hat{y}}$ ,è€Œä¸”å¯¹äºä»»æ„$\mathbf{\hat{y}}$,æœ‰$0\leq\mathbf{\hat{y}}\leq 1$

Â Â Â Â Â Â Â Â è€Œä¸”$\sum_i{\hat{y_i}} = 1$ï¼Œå¯ä»¥è§†ä¸ºä¸€ä¸ªæ­£ç¡®çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç»è¿‡è¯¥è¿ç®—ä¸ä¼šæ”¹å˜å„ä¸ªè¾“å‡ºä¹‹é—´çš„å¤§å°å…³ç³»ã€‚å› æ­¤æˆ‘ä»¬ä¾ç„¶å¯ä»¥è®¤ä¸º$max(\hat{y_i})$ä¸ºåˆ†ç±»ç»“æœã€‚

### æŸå¤±å‡½æ•°

Â Â Â Â Â Â Â Â æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥åº¦é‡é¢„æµ‹çš„æ•ˆæœã€‚æˆ‘ä»¬å°†ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡

~~å…¶å®çº¿æ€§å›å½’ç”¨çš„ä¹Ÿæ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡~~

> <mark>ä¸ºäº†é˜²æ­¢å¿˜è®°æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ­¥éª¤ï¼š</mark>
> 
> è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ›´ç®€å•çš„ä¾‹å­ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¡¬å¸ï¼Œæˆ‘ä»¬æƒ³è¦ä¼°è®¡è¿™ä¸ªç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ pã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸‰æ¬¡æŠ•æ·ï¼Œç»“æœåˆ†åˆ«æ˜¯æ­£é¢ã€åé¢ã€æ­£é¢ã€‚
> 
> æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼°è®¡ç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡ pã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªä¼¼ç„¶å‡½æ•°ï¼Œè¡¨ç¤ºè§‚æµ‹åˆ°è¿™ä¸‰æ¬¡ç»“æœçš„æ¦‚ç‡ï¼š
> 
> P(è§‚æµ‹æ•°æ®âˆ£p)=pâ‹…(1âˆ’p)â‹…p
> 
> è¿™é‡Œï¼Œp è¡¨ç¤ºæ­£é¢æœä¸Šçš„æ¦‚ç‡ï¼Œè€Œ 1âˆ’p è¡¨ç¤ºåé¢æœä¸Šçš„æ¦‚ç‡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿å¾—è§‚æµ‹æ•°æ®çš„ä¼¼ç„¶å‡½æ•°æœ€å¤§çš„ pã€‚
> 
> å–å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼ˆå¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„æœ€å¤§åŒ–ç­‰æ•ˆäºä¼¼ç„¶å‡½æ•°çš„æœ€å¤§åŒ–ï¼‰ï¼š
> 
> logP(è§‚æµ‹æ•°æ®âˆ£p)=log(p)+log(1âˆ’p)+log(p)
> 
> ç°åœ¨ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªå¯¹æ•°ä¼¼ç„¶å‡½æ•°å…³äº p æ±‚åå¯¼æ•°ï¼Œå¹¶ä»¤å…¶ç­‰äºé›¶ï¼Œè§£å‡º p çš„å€¼ã€‚åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚è§£è¿™ä¸ªæ–¹ç¨‹å¾—åˆ°æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç»“æœã€‚
> 
> dpdâ€‹logP(è§‚æµ‹æ•°æ®âˆ£p)=p1â€‹âˆ’1âˆ’p1â€‹+p1â€‹=0
> 
> è§£è¿™ä¸ªæ–¹ç¨‹ï¼Œæˆ‘ä»¬å¾—åˆ° p=$\frac{2}{3}$ã€‚æ‰€ä»¥ï¼Œæ ¹æ®è§‚æµ‹æ•°æ®ï¼Œç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚ç‡çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸º $\frac{2}{3}$â€‹ã€‚

å…ˆæ¨å¯¼ä¸€ä¸‹ï¼š

Â Â Â Â Â Â Â Â æˆ‘ä»¬å·²ç»ç”¨softmaxè·å¾—äº†ä¸€ä¸ªå‘é‡$\mathbf{\hat{y}}$ï¼Œå¯ä»¥å°†å…¶è§†å¯¹ç»™å®šä»»æ„è¾“å…¥$\mathbf{x}$çš„æ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡â€ã€‚æ¯”å¦‚$\hat{y_1}=P\{y=ğŸ±|\bf{x}\}$

Â Â Â Â Â Â Â Â é¦–å…ˆæˆ‘ä»¬æ¥æ±‚ä¼¼ç„¶å‡½æ•° ä¸º  $P(Y|X)=\prod_{i=1}^{n} P(\hat{y}^{(i)}|x^{(i)})$

Â Â Â Â Â Â Â Â å› ä¸ºè¿ä¹˜ä¼šå¯¼è‡´ä¼¼ç„¶å‡½æ•°å€¼è¿‡å°ï¼Œç”¨å¯¹æ•°åŒ–è¿ä¹˜ä¸ºæ±‚å’Œï¼ŒåŠ è´Ÿå·å³å¯é€šè¿‡æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹å¼è·å¾—æœ€å°å€¼ï¼Œå³ï¼š

$$
logP(Y|X)=\sum_{i=1}^{n} logP(\mathbf{\hat{y}}^{(i)}|\mathbf{x}^{(i)})  \\
$$

$$

-logP(Y|X)=\sum_{i=1}^{n} -logP(\mathbf{\hat{y}}^{(i)}|\mathbf{x}^{(i)}) =l(\bf{y},\bf{\hat{y}})

$$

åŒºåˆ«äºçº¿æ€§å›å½’ï¼Œsoftmaxæ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å¦‚æœè¦å°†æ ·æœ¬åˆ†ç±»ä¸ºğŸ±ï¼Œæˆ‘ä»¬éœ€è¦ğŸ±çš„æ¦‚ç‡å¤§äºğŸ¶å’ŒğŸ¥ï¼Œç”¨çº¿æ€§å›å½’çš„é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹å·®æ¥ä½œä¸ºæŸå¤±å‡½æ•°æ˜æ˜¾æ˜¯ä¸åˆé€‚çš„ã€‚

Â Â Â Â Â Â Â Â äºæ˜¯æˆ‘ä»¬å¼•å…¥`äº¤å‰ç†µæŸå¤±å‡½æ•°`

$$
l(\mathbf{y},\mathbf{\hat{y}})=H(y^{(i)},\hat{y}^{(i)})=\sum_{j=1}^qy_jlog\hat{y_j}
$$

è®¾$y_i$æŒ‡çš„æ˜¯çœŸå®å€¼çš„æ¦‚ç‡ï¼Œå‡å¦‚å·²çŸ¥ğŸ±ï¼Œä¸ºğŸ±çš„æ¦‚ç‡ä¸º1ï¼Œä¸ºğŸ¶ğŸ¥çš„æ¦‚ç‡ä¸º0

è®¾æ€»å…±æœ‰qä¸ªè¾“å‡ºï¼Œç”±$y_i=1$åªèƒ½æœ‰ä¸€ä¸ªï¼Œå…¶ä½™éƒ½ä¸º0

$$
H(y^{(i)},\hat{y}^{(i)})=-log\hat{y}_{y^{(i)}}^{(i)}
$$

> è§£é‡Šå½“$åªæœ‰å½“y_j=1æ—¶ï¼Œä¹Ÿå°±æ˜¯y_j=y_iæ—¶H\neq0ï¼Œä¹Ÿå°±æ˜¯log\hat{y}^{(i)}$
> 
> å‡å¦‚æœ‰ç‹¬çƒ­ç¼–ç  [0,1,0],æ¨¡å‹åœ¨æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­é¦–æ¬¡ç»™å‡ºçš„æ¦‚ç‡æ˜¯[0.6ï¼Œ0.1ï¼Œ0.3]ï¼Œç”±äºçœŸå®çš„åˆ†ç±»ä¸ºç¬¬äºŒç±»ï¼Œæ‰€ä»¥å°±ç®—æ¨¡å‹ç»™å‡ºç¬¬ä¸€ç±»çš„æ¦‚ç‡æœ€å¤§ï¼Œä¾ç„¶ä¼šé€‰æ‹©ç¬¬äºŒä¸ªæ¦‚ç‡åŠ å…¥æŸå¤±å‡½æ•°çš„è®¡ç®—ã€‚è¿™æ ·ä¼šé¼“åŠ±ç¬¬äºŒç±»çš„è¾“å‡ºæ¦‚ç‡æ›´æ¥è¿‘çœŸå®æ ‡ç­¾ã€‚
> 
> åŠ è´Ÿå·æ˜¯å› ä¸ºåœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œé€šå¸¸æ˜¯æœ€å°åŒ–æŸå¤±ã€‚åŠ è´Ÿå·å³å¯é€šè¿‡æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹å¼è·å¾—æœ€å°å€¼

## ä¼˜åŒ–ç®—æ³•

Â Â Â Â ç”±äºåœ¨å•ä¸ªè¾“å‡ºçš„è®¡ç®—æ–¹é¢ä¸çº¿æ€§å›å½’ç±»ä¼¼ï¼Œä¾ç„¶ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ–¹æ³•

Â Â Â Â Â å› æ­¤è¦å¯¹æŸå¤±å‡½æ•°æ±‚å¯¼ï¼š

$$
\begin{align*}
l(\mathbf{y}, \mathbf{\hat{y}}) &= -\sum_{j=1}^{q} y_j \log\left(\frac{\exp(o_j)}{\sum_{k=1}^{q} \exp(o_k)}\right) \\
&= -\sum_{j=1}^{q} y_j \log\left(\frac{\exp(o_j)}{\sum_{k=1}^{q} \exp(o_k)}\right) - \sum_{j=1}^{q} y_j o_j \\
&= \log\left(\sum_{k=1}^{q} \exp(o_k)\right) - \sum_{j=1}^{q} y_j o_j.
\end{align*}
$$

## è¯»å…¥æ•°æ®é›†

```python
  def get_fashion_mnist_labels(labels):
    """return text labels of Fashion-MNIST dataset """
    text_labels = [
        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',
        'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```

`return [text_labels[int(i)] for i in labels]`æ˜¯ä¸€ä¸ªåˆ—è¡¨æ¨å¯¼å¼ï¼Œå®ƒå°†è¾“å…¥çš„ `labels` åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´  `i` è½¬æ¢ä¸º `text_labels[int(i)]`ã€‚æœ€ç»ˆè¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰è½¬æ¢åå…ƒç´ çš„æ–°åˆ—è¡¨ã€‚

```python
def show_images(images, num_rows, num_cols, titles=None, scale=1.5):  
    figsize = (num_cols * scale, num_rows * scale)  
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)  
    axes = axes.flatten()  
    for i, (ax, img) in enumerate(zip(axes, images)):  
        ax.imshow(img,)  
        ax.axes.get_xaxis().set_visible(False)  
        ax.axes.get_yaxis().set_visible(False)  
        if titles:  
            ax.set_title(titles[i])  
    plt.show()
```

è¿™æ˜¯æ˜¾ç¤ºå›¾ç‰‡

```python
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
```

pytorchä¸­å›¾ç‰‡ç»å¸¸æ˜¯æœ‰ä¸‰ä¸ªç»´åº¦`(channels, height, width)`è¿™é‡Œchanelsæ²¡ç”¨ æ¢æˆäº†`batch_size`å¯ä»¥åœ¨`show_image`å‡½æ•°ä¸­ä¸€æ¬¡æ€§è¯»å…¥18å¼ å›¾ç‰‡ã€‚å¯ä»¥é€šè¿‡éå†`channels`

å±•ç¤ºæ‰€æœ‰å›¾ç‰‡ã€‚

```python
def get_dataloader_workers():
    return 4
train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, 
                              num_workers=get_dataloader_workers())
timer = d2l.Timer()
for X, y in train_iter:
    continue
print(f'{timer.stop():.2f} sec')
```

æ•°æ®è¿­ä»£å™¨ï¼Œå¹¶è®°å½•è¯»å–æ—¶é—´ï¼Œ`get_dataloader_workers`è®¾ç½®å¼€å¤šå°‘è¿›ç¨‹åŠ è½½æ•°æ®



## å…³é”®å‡½æ•°

### Softmaxè¿ç®—

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1,keepdim=True)
    return X_exp / partition
```

`partition = X_exp.sum(1,keepdim=True)`è¡¨ç¤ºæŒ‰ç…§æ¨ªå‘æ±‚å’Œï¼Œ~~ï¼ˆæ±‚è¡Œå’Œï¼‰~~

åšä¸ªå®éªŒï¼šæœ€ä¸Šé¢ä¸ºXï¼Œè‡ªä¸Šè€Œä¸‹åˆ†åˆ«æ˜¯`X.sum(0, keepdim=True)`

 Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Â `X.sum(1, keepdim=True)`

> ![](C:\Users\ASUS\AppData\Roaming\marktext\images\2024-01-27-21-31-30-image.png)
> 
> ä¸ºä»€ä¹ˆæ˜¯æ¨ªå‘ï¼Ÿ
> 
> å› ä¸ºè°ƒç”¨softmaxçš„æ˜¯æ¨¡å‹é¢„æµ‹å€¼ä¹Ÿå°±æ˜¯$\hat{\mathbf{y}}={\mathbf{O}}$è¿™é‡Œå‡è®¾nä¸ºæ ·æœ¬æ•°
> 
> $$
> \mathbf{O} = \mathbf{X}\mathbf{W}^T+\mathbf{B} = \begin{bmatrix}
     \mathbf{x_1W^T}+\mathbf{b_1}\\
     \mathbf{x_2W^T}+\mathbf{b_2}\\
...\\
     \mathbf{x_nW^T}+\mathbf{b_n}
                                            \end{bmatrix}
> $$
> 
> æ‰€ä»¥æœ€ç»ˆç»™å‡ºæ¨ªå‘æ˜¯åŒä¸€ä¸ªæ ·æœ¬ å¯¹äºä¸åŒæ ‡ç­¾çš„æ‰“åˆ†ã€‚
> 
> ![](C:\Users\ASUS\AppData\Roaming\marktext\images\2024-01-27-22-01-59-image.png)
> 
> éªŒè¯ä¸€ä¸‹ï¼šç”Ÿæˆä¸€ä¸ªäºŒè¡Œäº”åˆ—æ­£æ€åˆ†å¸ƒéšæœºçŸ©é˜µaï¼Œå°†å…¶è¿›è¡Œsoftmaxè¿ç®—ç»“æœå¦‚ä¸‹ï¼š
> 
> ![](C:\Users\ASUS\AppData\Roaming\marktext\images\2024-01-27-22-03-00-image.png)
> 
> æˆåŠŸè½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œä¸”æ¯è¡Œæ¦‚ç‡ä¹‹å’Œä¸º1ï¼Œæ˜¯æ­£ç¡®çš„

### Softmaxå›å½’æ¨¡å‹

```python
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

> è¿™é‡Œæ ¹æ®å…¬å¼å»ºç«‹èµ·äº†softmaxæ¨¡å‹ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯`X.reshape((-1,W.shape[0]))`è¿™é‡Œåº”è¯¥å°†X `reshape`æˆ`(batch_size,W.shape[0])`ï¼Œè¡Œå†™æˆ-1ä¼šè‡ªåŠ¨æ±‚è¯¥å¤„çš„å€¼~~å·æ‡’~~ã€‚
> 
> ç›´æ¥`+b`æ˜¯åˆ©ç”¨äº†å¹¿æ’­æœºåˆ¶ï¼Œ`b`è‡ªåŠ¨è¡¥é½ä¸ºç›¸åŒshapeçš„çŸ©é˜µã€‚

### äº¤å‰ç†µæŸå¤±

```python
y = torch.tensor([0, 2])  
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])  
print(y_hat)  
print(y_hat[[0,1], y])
```

> å…ˆçœ‹ä¸Šé¢è¿™ä¸²ä»£ç ğŸ‘†ï¼Œè¾“å‡ºç»“æœğŸ‘‡
> 
> ![](C:\Users\ASUS\AppData\Roaming\marktext\images\2024-01-27-23-18-32-image.png)
> 
> Â Â Â Â å‡è®¾ä¸€ç»´æ•°ç»„`y`æ˜¯æ ·æœ¬çœŸå®æ‰€å±åˆ†ç±»çš„ç´¢å¼•ï¼Œè½¬åŒ–ä¸ºğŸ±ğŸ¶ğŸ¥è¯­è¨€ï¼Œå°±ä»£è¡¨ç€ğŸ±ï¼ŒğŸ¥æ‰€åœ¨çš„ä½ç½®ã€‚`y_hat`æ˜¯æ¨¡å‹ç»™å‡ºçš„ä¸¤ä¸ªé¢„æµ‹æ¦‚ç‡ã€‚æ ¹æ®ä¹‹å‰çš„æ¨å¯¼ï¼Œäº¤å‰ç†µæŸå¤±è¦æ±‚æˆ‘ä»¬å¦‚æœçœŸå®çš„æ˜¯ğŸ±ï¼Œå°±ç®—æ¨¡å‹ç»™å‡ºğŸ±çš„æ¦‚ç‡æœ€å°ï¼Œæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ä¹Ÿè¿˜æ˜¯é€‰å–ä½œä¸ºğŸ±çš„æ¦‚ç‡ã€‚è¿™é‡Œè¡¨ç°ä¸ºçœŸå®çš„`y`ç´¢å¼•ä¸º`[0,2]`æ‰€ä»¥é€‰æ‹©ä½œä¸º`[0,2]`çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯`[0.1000, 0.5000]`ã€‚
> 
> Â Â Â Â è‡³äºä»£ç ä¸ºä»€ä¹ˆæ˜¯`y_hat[[0,1], y]`ï¼Œè¿™æ˜¯èŠ±å¼ç´¢å¼•`(fancy indexing)`,è¡¨ç¤ºä» `y_hat` ä¸­é€‰æ‹©ç¬¬ 0 è¡Œå’Œç¬¬ 1 è¡Œï¼ˆå¯¹åº”ä¸¤ä¸ªæ ·æœ¬ï¼‰,ç„¶ååˆ†åˆ«åœ¨ç¬¬0è¡Œçš„ç¬¬0ä¸ªé€‰å–å’Œç¬¬1è¡Œç¬¬äºŒä¸ªé€‰å–ã€‚


